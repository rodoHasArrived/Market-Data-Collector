# Consolidated Documentation, AI Instruction Sync & TODO Automation
#
# Replaces and combines:
#   - docs-comprehensive.yml (Documentation & Workflow Automation)
#   - docs-auto-update.yml (Docs Auto-Update)
#   - docs-structure-sync.yml (Docs Structure Sync)
#   - ai-instructions-sync.yml (AI Instructions Sync)
#   - todo-automation.yml (TODO Scanning & Documentation)
#
# All documentation generation, validation, AI instruction sync,
# known-error intake, and TODO scanning is unified here.
# No functionality removed.
#
# TODO Lifecycle Management includes:
#   - Core TODO scanning and documentation
#   - Trend tracking (historical TODO count analysis)
#   - Stale TODO detection (git blame-based aging)
#   - Resolved issue audit (orphaned TODOs)
#   - PR TODO diff (new TODO detection in PRs)
#   - Duplicate detection (similar TODO identification)
#   - Hotspot analysis (TODO density by file/directory)

name: Documentation Automation

on:
  push:
    branches: ["main"]
    paths:
      - 'docs/**'
      - '*.md'
      - 'CLAUDE.md'
      - '.github/workflows/**'
      - '.github/agents/**'
      - 'build/scripts/docs/**'
      - 'build/dotnet/DocGenerator/**'
      - 'build/**/*.py'
      - 'src/**'
      - 'tests/**'
      - 'benchmarks/**'
      - 'config/appsettings.sample.json'
  pull_request:
    branches: ["main"]
    paths:
      - 'docs/**'
      - '*.md'
      - 'CLAUDE.md'
      - '.github/workflows/**'
      - '.github/agents/**'
      - 'build/scripts/docs/**'
      - 'build/dotnet/DocGenerator/**'
      - 'src/**'
  schedule:
    - cron: '0 3 * * 1'
  workflow_dispatch:
    inputs:
      regenerate:
        description: 'Force regenerate generated docs and AI instruction files'
        required: false
        default: 'false'
        type: boolean
      update_all:
        description: 'Update all auto-generated documentation outputs'
        required: false
        default: 'false'
        type: boolean
      dry_run:
        description: 'Show changes without committing'
        required: false
        default: 'true'
        type: boolean
      create_pr:
        description: 'Create PR for generated updates (manual runs only)'
        required: false
        default: 'false'
        type: boolean
      issue_number:
        description: 'Optional issue number to ingest into docs/ai-known-errors.md'
        required: false
        type: string
      force_update:
        description: 'Force regenerate all structure documentation'
        required: false
        default: 'false'
        type: boolean
      scan_todos:
        description: 'Run TODO scan and documentation update'
        required: false
        default: 'true'
        type: boolean
      create_issues:
        description: 'Create GitHub issues for untracked TODOs'
        required: false
        default: false
        type: boolean
      include_notes:
        description: 'Include NOTE comments in TODO documentation'
        required: false
        default: true
        type: boolean
      use_copilot:
        description: 'Use AI to generate TODO triage recommendations'
        required: false
        default: true
        type: boolean
      stale_threshold_days:
        description: 'Days before a TODO is considered stale'
        required: false
        default: 90
        type: number
      close_resolved_issues:
        description: 'Auto-comment on TODOs whose linked issues are closed'
        required: false
        default: false
        type: boolean
  issues:
    types: [opened, edited, labeled, reopened]

permissions:
  contents: write
  issues: write
  pull-requests: write
  models: read

concurrency:
  group: documentation-${{ github.ref }}
  cancel-in-progress: true

defaults:
  run:
    shell: bash

env:
  DOTNET_VERSION: '9.0.x'
  PYTHON_VERSION: '3.11'
  DOTNET_NOLOGO: true
  DOTNET_CLI_TELEMETRY_OPTOUT: true

jobs:
  # â”€â”€â”€ AI Known Errors Intake â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ai-known-errors-intake:
    name: AI Known Errors Intake
    if: |
      (github.event_name == 'issues' && contains(github.event.issue.labels.*.name, 'ai-known-error')) ||
      (github.event_name == 'workflow_dispatch' && github.event.inputs.issue_number != '' && github.event.inputs.issue_number != null)
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Resolve issue payload for workflow_dispatch
        if: github.event_name == 'workflow_dispatch'
        id: fetch_issue
        uses: actions/github-script@v7
        with:
          script: |
            const issue_number = Number(core.getInput('issue_number'));
            if (!issue_number) {
              core.setFailed('workflow_dispatch requires a numeric issue_number input for intake.');
              return;
            }

            const { data } = await github.rest.issues.get({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number,
            });

            core.setOutput('number', String(data.number));
            core.setOutput('title', data.title || '');
            core.setOutput('body', data.body || '');

      - name: Update docs/ai-known-errors.md from issue
        env:
          EVENT_NAME: ${{ github.event_name }}
          ISSUE_NUMBER: ${{ github.event.issue.number }}
          ISSUE_TITLE: ${{ github.event.issue.title }}
          ISSUE_BODY: ${{ github.event.issue.body }}
          DISPATCH_ISSUE_NUMBER: ${{ steps.fetch_issue.outputs.number }}
          DISPATCH_ISSUE_TITLE: ${{ steps.fetch_issue.outputs.title }}
          DISPATCH_ISSUE_BODY: ${{ steps.fetch_issue.outputs.body }}
        run: |
          python3 - <<'PY'
          import datetime
          import os
          import pathlib
          import re

          def text(name: str) -> str:
              return os.environ.get(name, "") or ""

          event_name = text("EVENT_NAME")
          if event_name == "workflow_dispatch":
              number = text("DISPATCH_ISSUE_NUMBER")
              title = text("DISPATCH_ISSUE_TITLE")
              body = text("DISPATCH_ISSUE_BODY")
          else:
              number = text("ISSUE_NUMBER")
              title = text("ISSUE_TITLE")
              body = text("ISSUE_BODY")

          if not number:
              raise SystemExit("No issue number found.")

          doc = pathlib.Path("docs/ai-known-errors.md")
          current = doc.read_text(encoding="utf-8")
          marker = f"#${number}"
          if marker in current:
              print(f"Entry already exists for issue {number}; skipping")
              raise SystemExit(0)

          def section(name: str, fallback: str) -> str:
              m = re.search(rf"(?ims)^##\s*{re.escape(name)}\s*$\n(.*?)(?=^##\s+|\Z)", body)
              if not m:
                  return fallback
              value = m.group(1).strip()
              return value if value else fallback

          def normalize_lines(value: str) -> str:
              lines = [ln.rstrip() for ln in value.splitlines() if ln.strip()]
              return "\n".join(lines) if lines else "Not provided."

          def checkbox_lines(value: str) -> str:
              items = []
              for ln in value.splitlines():
                  s = ln.strip()
                  if not s:
                      continue
                  s = re.sub(r"^[-*]\s*", "", s)
                  s = re.sub(r"^\[[ xX]\]\s*", "", s)
                  items.append(f"  - [ ] {s}")
              if not items:
                  items = ["  - [ ] Add prevention checks from the linked issue."]
              return "\n".join(items)

          def command_lines(value: str) -> str:
              cmds = []
              in_code = False
              for ln in value.splitlines():
                  if ln.strip().startswith("```"):
                      in_code = not in_code
                      continue
                  s = ln.strip()
                  if not s:
                      continue
                  if in_code:
                      cmds.append(f"  - `{s}`")
                  elif s.startswith("-"):
                      cmds.append(f"  - `{s.lstrip('- ').strip('`')}`")
                  elif re.match(r"^[A-Za-z0-9_.\-/]+", s):
                      cmds.append(f"  - `{s.strip('`')}`")
              if not cmds:
                  cmds = ["  - `# add verification commands`"]
              return "\n".join(cmds)

          area = normalize_lines(section("Area", "process"))
          symptoms = normalize_lines(section("Symptoms", title or "Issue description not provided."))
          root = normalize_lines(section("Root cause", "Root cause not yet documented."))
          prevention = checkbox_lines(section("Prevention checklist", ""))
          verification = command_lines(section("Verification commands", ""))
          references = normalize_lines(section("References", f"- https://github.com/${{ github.repository }}/issues/{number}"))

          today = datetime.datetime.utcnow().date().isoformat()

          block = f"""
          ## [{today}] #{number} {title or 'Untitled issue'}
          - Area: {area}
          - Symptoms:
            {symptoms.replace(chr(10), chr(10) + '  ')}
          - Root cause:
            {root.replace(chr(10), chr(10) + '  ')}
          - Prevention checklist:
          {prevention}
          - Verification commands:
          {verification}
          - References:
            {references.replace(chr(10), chr(10) + '  ')}
          """.strip()

          updated = current.rstrip() + "\n\n" + block + "\n"
          doc.write_text(updated, encoding="utf-8")
          print(f"Appended issue {number} to {doc}")
          PY

      - name: Check for changes
        id: changes
        run: |
          if git diff --quiet -- docs/ai-known-errors.md; then
            echo "has_changes=false" >> "$GITHUB_OUTPUT"
          else
            echo "has_changes=true" >> "$GITHUB_OUTPUT"
          fi

      - name: Create PR with known-error update
        if: steps.changes.outputs.has_changes == 'true'
        uses: peter-evans/create-pull-request@v7
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          commit-message: "docs: ingest ai-known-error issue into registry"
          title: "docs: ingest ai-known-error issue into registry"
          body: |
            ## Summary
            - ingest a labeled AI issue into `docs/ai-known-errors.md`
            - preserve source issue reference for traceability

            ## Trigger
            - `${{ github.event_name }}` event
          branch: automation/ai-known-errors-intake
          delete-branch: true
          labels: |
            documentation
            automation

  # â”€â”€â”€ Change Detection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  detect-changes:
    name: Detect Documentation & Structure Changes
    if: github.event_name != 'issues'
    runs-on: ubuntu-latest
    timeout-minutes: 10
    outputs:
      docs_changed: ${{ steps.filter.outputs.docs }}
      structure_changed: ${{ steps.filter.outputs.structure }}
      workflow_changed: ${{ steps.filter.outputs.workflows }}
      providers_changed: ${{ steps.filter.outputs.providers }}
      interfaces_changed: ${{ steps.filter.outputs.interfaces }}
      adrs_changed: ${{ steps.filter.outputs.adrs }}
      config_changed: ${{ steps.filter.outputs.config }}
      microservices_changed: ${{ steps.filter.outputs.microservices }}
      ai_files_changed: ${{ steps.filter.outputs.ai_files }}
      src_changed: ${{ steps.filter.outputs.src }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 2

      - name: Detect changed areas
        id: filter
        uses: dorny/paths-filter@v3.0.2
        with:
          filters: |
            docs:
              - 'docs/**'
              - '*.md'
              - 'CLAUDE.md'
            workflows:
              - '.github/workflows/**'
            structure:
              - 'src/**'
              - 'tests/**'
              - 'benchmarks/**'
              - 'build/**'
              - added|deleted: 'src/**'
              - added|deleted: 'docs/**'
            providers:
              - 'src/MarketDataCollector/Infrastructure/Providers/**'
            interfaces:
              - 'src/MarketDataCollector/Infrastructure/IMarketDataClient.cs'
              - 'src/MarketDataCollector/Infrastructure/Providers/Backfill/IHistoricalDataProvider*.cs'
            adrs:
              - 'docs/adr/**'
            config:
              - 'config/appsettings.sample.json'
            microservices:
              - 'src/Microservices/**'
            ai_files:
              - 'docs/ai/**'
              - '.github/agents/**'
              - 'CLAUDE.md'
            src:
              - 'src/**/*.cs'
              - 'src/**/*.fs'

  # â”€â”€â”€ Validate Documentation Quality â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  validate-docs:
    name: Validate Documentation Quality
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: detect-changes
    if: |
      needs.detect-changes.outputs.docs_changed == 'true' ||
      needs.detect-changes.outputs.workflow_changed == 'true' ||
      github.event_name == 'workflow_dispatch' ||
      github.event_name == 'schedule'
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Markdown lint
        uses: DavidAnson/markdownlint-cli2-action@v22.0.0
        with:
          globs: |
            **/*.md
            !**/node_modules/**
            !**/bin/**
            !**/obj/**
        continue-on-error: true

      - name: Link validation
        uses: gaurav-nelson/github-action-markdown-link-check@1.0.17
        with:
          use-quiet-mode: 'yes'
          use-verbose-mode: 'no'
          folder-path: 'docs/'
          max-depth: 5
        continue-on-error: true

      - name: ADR inventory
        run: |
          set -euo pipefail
          echo "## ADR Verification Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          ADR_COUNT=$(find docs -path "*/adr/*.md" -o -path "*/decisions/*.md" 2>/dev/null | wc -l)
          echo "Found $ADR_COUNT ADR files" >> $GITHUB_STEP_SUMMARY

  # â”€â”€â”€ Regenerate All Documentation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  regenerate-docs:
    name: Regenerate Docs, Structure & AI Instructions
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: detect-changes
    if: |
      needs.detect-changes.outputs.structure_changed == 'true' ||
      needs.detect-changes.outputs.docs_changed == 'true' ||
      needs.detect-changes.outputs.workflow_changed == 'true' ||
      needs.detect-changes.outputs.providers_changed == 'true' ||
      needs.detect-changes.outputs.interfaces_changed == 'true' ||
      needs.detect-changes.outputs.adrs_changed == 'true' ||
      needs.detect-changes.outputs.config_changed == 'true' ||
      needs.detect-changes.outputs.microservices_changed == 'true' ||
      needs.detect-changes.outputs.ai_files_changed == 'true' ||
      needs.detect-changes.outputs.src_changed == 'true' ||
      github.event.inputs.update_all == 'true' ||
      github.event_name == 'workflow_dispatch' ||
      github.event_name == 'schedule'
    outputs:
      has_changes: ${{ steps.changes.outputs.has_changes }}
      changed_files: ${{ steps.changes.outputs.changed_files }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup Python
        uses: actions/setup-python@v6.2.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Setup .NET with Cache
        uses: ./.github/actions/setup-dotnet-cache
        with:
          dotnet-version: ${{ env.DOTNET_VERSION }}

      - name: Build project for metadata extraction
        run: |
          set -euo pipefail
          dotnet restore MarketDataCollector.sln /p:EnableWindowsTargeting=true
          dotnet build src/MarketDataCollector/MarketDataCollector.csproj \
            -c Release \
            --no-restore \
            /p:EnableWindowsTargeting=true \
            /p:GenerateDocumentationFile=true
        continue-on-error: true

      # --- Structure docs ---
      - name: Generate structure and workflow docs
        run: |
          set -euo pipefail
          mkdir -p docs/generated
          python3 build/scripts/docs/generate-structure-docs.py --output docs/generated/repository-structure.md --format markdown
          python3 build/scripts/docs/generate-structure-docs.py --output docs/generated/workflows-overview.md --workflows-only

      # --- Provider docs ---
      - name: Generate provider registry
        if: |
          needs.detect-changes.outputs.providers_changed == 'true' ||
          needs.detect-changes.outputs.interfaces_changed == 'true' ||
          github.event.inputs.update_all == 'true' ||
          github.event.inputs.force_update == 'true' ||
          github.event_name == 'schedule'
        run: |
          set -euo pipefail
          python3 build/scripts/docs/generate-structure-docs.py \
            --output docs/generated/provider-registry.md \
            --providers-only \
            --extract-attributes

      # --- ADR compliance and index ---
      - name: Verify ADR compliance
        if: |
          needs.detect-changes.outputs.adrs_changed == 'true' ||
          github.event.inputs.update_all == 'true' ||
          github.event_name == 'schedule'
        run: |
          set -euo pipefail
          if [ -d "build/dotnet/DocGenerator" ]; then
            dotnet run --project build/dotnet/DocGenerator/DocGenerator.csproj --no-build -c Release -- verify-adrs \
              --adr-dir docs/adr \
              --src-dir . || true
          fi

      - name: Generate ADR index
        run: |
          set -euo pipefail
          mkdir -p docs/generated
          echo "# Architecture Decision Records" > docs/generated/adr-index.md
          echo "" >> docs/generated/adr-index.md
          echo "> Auto-generated on $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> docs/generated/adr-index.md
          echo "" >> docs/generated/adr-index.md
          echo "| ADR | Title | Status |" >> docs/generated/adr-index.md
          echo "|-----|-------|--------|" >> docs/generated/adr-index.md

          for adr in docs/adr/*.md; do
            if [ -f "$adr" ]; then
              filename=$(basename "$adr")
              title=$(head -1 "$adr" | sed 's/^# //')
              status="Accepted"
              if grep -qi "status:.*superseded" "$adr"; then
                status="Superseded"
              elif grep -qi "status:.*deprecated" "$adr"; then
                status="Deprecated"
              fi
              echo "| [$filename](../adr/$filename) | $title | $status |" >> docs/generated/adr-index.md
            fi
          done

          echo "" >> docs/generated/adr-index.md
          echo "---" >> docs/generated/adr-index.md
          echo "*This file is auto-generated. Do not edit manually.*" >> docs/generated/adr-index.md

      # --- Configuration schema ---
      - name: Generate configuration schema
        run: |
          set -euo pipefail
          mkdir -p docs/generated

          cat > docs/generated/configuration-schema.md << 'EOD'
          # Configuration Schema

          > Auto-generated from `config/appsettings.sample.json`

          This document describes the configuration options available in the Market Data Collector.

          ## Configuration Sections

          EOD

          if [ -f "config/appsettings.sample.json" ]; then
            python3 -c "
          import json
          import sys

          try:
              with open('config/appsettings.sample.json', 'r') as f:
                  config = json.load(f)

              for section, value in config.items():
                  print(f'### {section}')
                  print('')
                  if isinstance(value, dict):
                      print('| Setting | Type | Description |')
                      print('|---------|------|-------------|')
                      for key, val in value.items():
                          val_type = type(val).__name__
                          print(f'| \`{key}\` | {val_type} | - |')
                  else:
                      print(f'Type: \`{type(value).__name__}\`')
                  print('')
          except Exception as e:
              print(f'Error parsing config: {e}', file=sys.stderr)
          " >> docs/generated/configuration-schema.md
          fi

          echo "---" >> docs/generated/configuration-schema.md
          echo "*This file is auto-generated. Do not edit manually.*" >> docs/generated/configuration-schema.md

      # --- Project context ---
      - name: Generate project context doc
        run: |
          set -euo pipefail
          if [ -d "build/dotnet/DocGenerator" ]; then
            dotnet restore build/dotnet/DocGenerator/DocGenerator.csproj
            dotnet build build/dotnet/DocGenerator/DocGenerator.csproj -c Release -v q || true
            dotnet run --project build/dotnet/DocGenerator/DocGenerator.csproj --no-build -c Release -- context --src src/MarketDataCollector --output docs/generated/project-context.md || true
          fi

      # --- AI instruction sync ---
      - name: Validate documentation automation inputs
        run: |
          set -euo pipefail
          required=(
            "build/scripts/docs/generate-structure-docs.py"
            "build/scripts/docs/update-claude-md.py"
            "docs/generated/repository-structure.md"
          )

          for path in "${required[@]}"; do
            if [ ! -e "$path" ]; then
              echo "Missing required file: $path" >&2
              exit 1
            fi
          done

      - name: Sync AI instruction files
        run: |
          set -euo pipefail
          files=(
            "CLAUDE.md"
            "docs/ai/copilot/instructions.md"
            ".github/agents/documentation-agent.md"
          )

          for file in "${files[@]}"; do
            if [ -f "$file" ]; then
              python3 build/scripts/docs/update-claude-md.py \
                --claude-md "$file" \
                --structure-source docs/generated/repository-structure.md
            else
              echo "Skipping missing AI instruction file: $file"
            fi
          done

      # --- AI-powered documentation quality review ---
      - name: AI documentation quality review
        id: ai-doc-review
        if: |
          github.event_name == 'schedule' ||
          (github.event_name == 'workflow_dispatch' && github.event.inputs.regenerate == 'true')
        continue-on-error: true
        uses: actions/ai-inference@v1
        with:
          model: openai/gpt-4.1-mini
          max-tokens: 1500
          prompt: |
            You are a documentation quality reviewer for a .NET market data collection project.
            Review the following generated documentation file list and provide:
            1) A brief quality assessment (completeness, consistency)
            2) Any gaps in documentation coverage
            3) Suggestions for improvement (max 3)

            Keep the response under 200 words in markdown format.

            Generated documentation files:
            - docs/generated/repository-structure.md
            - docs/generated/provider-registry.md
            - docs/generated/workflows-overview.md
            - docs/generated/project-context.md
            - docs/generated/adr-index.md
            - docs/generated/configuration-schema.md

            AI instruction files synced:
            - CLAUDE.md
            - docs/ai/copilot/instructions.md
            - .github/agents/documentation-agent.md

      # --- Check for changes ---
      - name: Check for generated changes
        id: changes
        run: |
          set -euo pipefail
          watched=(
            "docs/generated/"
            "docs/generated/adr-index.md"
            "docs/generated/configuration-schema.md"
            "CLAUDE.md"
            "docs/ai/copilot/instructions.md"
            ".github/agents/documentation-agent.md"
          )

          changed_files=$(git status --porcelain "${watched[@]}" | awk '{print $2}' | paste -sd ',' -)

          if [ -n "$(git status --porcelain "${watched[@]}")" ]; then
            echo "has_changes=true" >> $GITHUB_OUTPUT
            echo "changed_files=$changed_files" >> $GITHUB_OUTPUT
            {
              echo "### Generated files changed"
              git status --short -- "${watched[@]}"
            } >> "$GITHUB_STEP_SUMMARY"
          else
            echo "has_changes=false" >> $GITHUB_OUTPUT
            echo "changed_files=" >> $GITHUB_OUTPUT
            echo "No generated documentation changes detected." >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Append AI review to summary
        if: always() && steps.ai-doc-review.outputs.response != ''
        run: |
          {
            echo ""
            echo "### AI Documentation Quality Review"
            echo ""
            echo "${{ steps.ai-doc-review.outputs.response }}"
          } >> "$GITHUB_STEP_SUMMARY"

      - name: Show dry-run diff
        if: |
          steps.changes.outputs.has_changes == 'true' &&
          github.event_name == 'workflow_dispatch' &&
          github.event.inputs.dry_run == 'true'
        run: |
          set -euo pipefail
          git --no-pager diff -- docs/generated/ CLAUDE.md docs/ai/copilot/instructions.md .github/agents/documentation-agent.md

      - name: Upload dry-run patch artifact
        if: |
          steps.changes.outputs.has_changes == 'true' &&
          github.event_name == 'workflow_dispatch' &&
          github.event.inputs.dry_run == 'true'
        run: |
          set -euo pipefail
          git --no-pager diff --binary -- docs/generated/ CLAUDE.md docs/ai/copilot/instructions.md .github/agents/documentation-agent.md > docs-automation.patch

      - name: Publish dry-run patch artifact
        if: |
          steps.changes.outputs.has_changes == 'true' &&
          github.event_name == 'workflow_dispatch' &&
          github.event.inputs.dry_run == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: docs-automation-diff
          path: docs-automation.patch
          if-no-files-found: error

      - name: Commit direct updates
        if: |
          steps.changes.outputs.has_changes == 'true' &&
          (github.event_name == 'push' || github.event_name == 'schedule') &&
          github.actor != 'github-actions[bot]'
        run: |
          set -euo pipefail
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git add docs/generated/ CLAUDE.md docs/ai/copilot/instructions.md .github/agents/documentation-agent.md
          git commit -m "docs: consolidated documentation automation updates

          - Regenerated docs/generated artifacts
          - Synced AI instruction files
          - Updated provider registry, ADR index, config schema

          [skip ci]"
          git push

      - name: Create pull request for manual updates
        id: create-pr
        if: |
          steps.changes.outputs.has_changes == 'true' &&
          github.event_name == 'workflow_dispatch' &&
          github.event.inputs.create_pr == 'true' &&
          github.event.inputs.dry_run != 'true'
        continue-on-error: true
        uses: peter-evans/create-pull-request@v7
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          branch: automation/docs-comprehensive-sync
          delete-branch: true
          commit-message: "docs: consolidated documentation automation updates"
          title: "docs: consolidated documentation automation updates"
          body: |
            ## Summary
            - Regenerated `docs/generated/*` artifacts.
            - Updated `docs/generated/adr-index.md` from ADR source files.
            - Updated `docs/generated/configuration-schema.md` from sample configuration.
            - Synced repository-structure sections in AI instruction files.
            - Updated provider registry, ADR index, configuration schema.
            - Consolidated all documentation automation outputs into one run.
          labels: |
            documentation
            automation

      - name: Fallback - Commit directly if PR creation failed
        if: |
          steps.changes.outputs.has_changes == 'true' &&
          github.event_name == 'workflow_dispatch' &&
          github.event.inputs.create_pr == 'true' &&
          github.event.inputs.dry_run != 'true' &&
          steps.create-pr.outcome == 'failure'
        run: |
          echo "::warning::PR creation failed. Falling back to direct commit."
          echo "::notice::To enable PR creation, go to Settings > Actions > General and enable 'Allow GitHub Actions to create and approve pull requests'"

          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git add docs/generated/ CLAUDE.md docs/ai/copilot/instructions.md .github/agents/documentation-agent.md
          git commit -m "docs: consolidated documentation automation updates

          Note: Committed directly because PR creation is disabled in repository settings.

          [skip ci]"

          git push

  # â”€â”€â”€ TODO Scanning and Documentation (from todo-automation.yml) â”€â”€â”€â”€â”€â”€
  scan-todos:
    name: Scan and Document TODOs
    needs: [detect-changes, regenerate-docs]
    if: |
      always() &&
      needs.detect-changes.result == 'success' &&
      github.event_name != 'issues' && (
        needs.detect-changes.outputs.src_changed == 'true' ||
        needs.detect-changes.outputs.structure_changed == 'true' ||
        github.event_name == 'schedule' ||
        (github.event_name == 'workflow_dispatch' && github.event.inputs.scan_todos != 'false')
      )
    runs-on: ubuntu-latest
    timeout-minutes: 15
    outputs:
      total_count: ${{ steps.scan.outputs.total_count }}
      new_todos: ${{ steps.scan.outputs.new_todos }}
      has_changes: ${{ steps.check-changes.outputs.has_changes }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Pull latest changes
        run: |
          git pull origin ${{ github.ref_name }} --rebase || true

      - name: Set up Python
        uses: actions/setup-python@v6.2.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Scan codebase for TODOs
        id: scan
        run: |
          python3 build/scripts/docs/scan-todos.py \
            --output docs/status/TODO.md \
            --format markdown \
            --include-notes ${{ github.event.inputs.include_notes || 'true' }} \
            --json-output todo-scan-results.json

          # Extract metrics for workflow outputs
          if [ -f todo-scan-results.json ]; then
            TOTAL=$(python3 -c "import json; data=json.load(open('todo-scan-results.json')); print(data.get('total_count', 0))")
            NEW=$(python3 -c "import json; data=json.load(open('todo-scan-results.json')); print(len([t for t in data.get('todos', []) if not t.get('has_issue', False)]))")
            SCAN_JSON=$(python3 -c "import json; data=json.load(open('todo-scan-results.json')); print(json.dumps(data, separators=(',', ':')))")
            echo "total_count=$TOTAL" >> $GITHUB_OUTPUT
            echo "new_todos=$NEW" >> $GITHUB_OUTPUT
            echo "scan_json=$SCAN_JSON" >> $GITHUB_OUTPUT
          else
            echo "total_count=0" >> $GITHUB_OUTPUT
            echo "new_todos=0" >> $GITHUB_OUTPUT
            echo 'scan_json={"total_count":0,"todos":[]}' >> $GITHUB_OUTPUT
          fi

      - name: Generate Copilot triage recommendations
        id: copilot
        if: github.event.inputs.use_copilot != 'false'
        continue-on-error: true
        uses: actions/ai-inference@v1
        with:
          model: openai/gpt-4.1-mini
          max-tokens: 1800
          prompt: |
            You are an engineering assistant helping triage TODO comments in a repository.
            Produce a concise markdown report with these sections:
            1) Highest-priority TODOs (max 5)
            2) Recommended issue labels
            3) Immediate risk items
            4) Suggested batching plan

            Keep the output under 300 words.
            Use this scan JSON:
            ${{ steps.scan.outputs.scan_json }}

      - name: Build fallback triage report
        if: always()
        run: |
          set -euo pipefail

          python3 << 'EOF'
          import json
          from pathlib import Path

          scan_path = Path('todo-scan-results.json')
          report_path = Path('todo-triage.md')

          if not scan_path.exists():
              report_path.write_text('# TODO Triage\n\nNo scan results were found.\n', encoding='utf-8')
              raise SystemExit(0)

          data = json.loads(scan_path.read_text(encoding='utf-8'))
          todos = data.get('todos', [])
          untracked = [t for t in todos if not t.get('has_issue', False)]

          lines = [
              '# TODO Triage Recommendations',
              '',
              '_Generated by Documentation Automation workflow (TODO scan). Copilot output is appended when available; otherwise this deterministic fallback is produced._',
              '',
              f"- Total TODOs: **{data.get('total_count', 0)}**",
              f"- Untracked TODOs: **{len(untracked)}**",
              '',
              '## Highest-priority TODOs (fallback)',
              ''
          ]

          for i, todo in enumerate(untracked[:5], 1):
              todo_type = todo.get('type', 'TODO')
              text = todo.get('text', 'No description')
              file_path = todo.get('file', 'unknown')
              line = todo.get('line', 0)
              lines.append(f"{i}. **[{todo_type}]** `{file_path}:{line}` â€” {text}")

          if not untracked:
              lines.append('- No untracked TODO items found.')

          lines.extend([
              '',
              '## Suggested labels',
              '',
              '- `todo-automation` for all imported TODO items',
              '- `bug` for FIXME items',
              '- `tech-debt` for HACK items',
              '',
              '## Suggested batching plan',
              '',
              '1. Resolve untracked FIXME items first.',
              '2. Group TODO/HACK items by directory owner.',
              '3. Convert remaining high-value TODOs into tracked issues.'
          ])

          report_path.write_text('\n'.join(lines) + '\n', encoding='utf-8')
          EOF

      - name: Append Copilot triage output to report
        if: always() && steps.copilot.outputs.response != ''
        run: |
          {
            echo ""
            echo "## Copilot Recommendations"
            echo ""
            echo "${{ steps.copilot.outputs.response }}"
          } >> todo-triage.md

      - name: Check for changes
        id: check-changes
        run: |
          git add docs/status/TODO.md
          if git diff --staged --quiet; then
            echo "has_changes=false" >> $GITHUB_OUTPUT
            echo "No changes to TODO documentation"
          else
            echo "has_changes=true" >> $GITHUB_OUTPUT
            echo "Changes detected in TODO documentation"
          fi

      - name: Generate summary
        run: |
          echo "## TODO Scan Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f todo-scan-results.json ]; then
            python3 << 'EOF'
          import json
          import os

          with open('todo-scan-results.json', 'r') as f:
              data = json.load(f)

          summary = []
          summary.append(f"**Total TODOs:** {data.get('total_count', 0)}")
          summary.append(f"**Linked to Issues:** {len([t for t in data.get('todos', []) if t.get('has_issue', False)])}")
          summary.append(f"**Untracked:** {len([t for t in data.get('todos', []) if not t.get('has_issue', False)])}")
          summary.append("")

          # By type
          by_type = {}
          for todo in data.get('todos', []):
              t = todo.get('type', 'TODO')
              by_type[t] = by_type.get(t, 0) + 1

          if by_type:
              summary.append("### By Type")
              summary.append("| Type | Count |")
              summary.append("|------|-------|")
              for t, count in sorted(by_type.items()):
                  summary.append(f"| {t} | {count} |")
              summary.append("")

          # By directory
          by_dir = {}
          for todo in data.get('todos', []):
              path = todo.get('file', '')
              parts = path.split('/')
              if len(parts) > 1:
                  dir_name = parts[0] if parts[0] != '.' else parts[1] if len(parts) > 1 else 'root'
              else:
                  dir_name = 'root'
              by_dir[dir_name] = by_dir.get(dir_name, 0) + 1

          if by_dir:
              summary.append("### By Directory")
              summary.append("| Directory | Count |")
              summary.append("|-----------|-------|")
              for d, count in sorted(by_dir.items(), key=lambda x: -x[1]):
                  summary.append(f"| `{d}/` | {count} |")

          with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as f:
              f.write('\n'.join(summary))
          EOF
          else
            echo "No TODO scan results found." >> $GITHUB_STEP_SUMMARY
          fi

          if [ -f todo-triage.md ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Triage Recommendations" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            cat todo-triage.md >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload scan results
        uses: actions/upload-artifact@v4
        with:
          name: todo-scan-results
          path: |
            todo-scan-results.json
            docs/status/TODO.md
            todo-triage.md
          retention-days: 30

      - name: Commit changes
        if: |
          steps.check-changes.outputs.has_changes == 'true' &&
          github.event.inputs.dry_run != 'true' &&
          (github.event_name == 'push' || github.event_name == 'schedule' ||
           (github.event_name == 'workflow_dispatch' && github.event.inputs.dry_run != 'true'))
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git add docs/status/TODO.md
          git commit -m "docs: update TODO documentation [skip ci]

          Auto-generated TODO tracking documentation.

          - Total TODOs: ${{ steps.scan.outputs.total_count }}
          - Untracked: ${{ steps.scan.outputs.new_todos }}

          Generated by Documentation Automation workflow."

          git push

  # â”€â”€â”€ Create Issues for Untracked TODOs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  create-todo-issues:
    name: Create Issues for Untracked TODOs
    needs: scan-todos
    if: |
      github.event_name == 'workflow_dispatch' &&
      github.event.inputs.create_issues == 'true' &&
      needs.scan-todos.outputs.new_todos > 0
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download scan results
        uses: actions/download-artifact@v4
        with:
          name: todo-scan-results

      - name: Set up Python
        uses: actions/setup-python@v6.2.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Create issues for untracked TODOs
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          python3 << 'EOF'
          import json
          import subprocess
          import os

          with open('todo-scan-results.json', 'r') as f:
              data = json.load(f)

          created_count = 0
          max_issues = 5  # Limit to prevent spam

          for todo in data.get('todos', []):
              if created_count >= max_issues:
                  print(f"Reached maximum issue limit ({max_issues})")
                  break

              if todo.get('has_issue', False):
                  continue

              todo_type = todo.get('type', 'TODO')
              file_path = todo.get('file', 'unknown')
              line = todo.get('line', 0)
              text = todo.get('text', 'No description')

              # Determine labels
              labels = ['todo-automation']
              if todo_type == 'FIXME':
                  labels.append('bug')
              elif todo_type == 'HACK':
                  labels.append('tech-debt')

              title = f"[{todo_type}] {text[:60]}{'...' if len(text) > 60 else ''}"

              body = f"""## TODO Item

          **Type:** {todo_type}
          **File:** `{file_path}`
          **Line:** {line}

          ### Description

          {text}

          ### Context

          ```
          {todo.get('context', 'No context available')}
          ```

          ---
          *This issue was automatically created by the Documentation Automation workflow.*
          """

              try:
                  result = subprocess.run([
                      'gh', 'issue', 'create',
                      '--title', title,
                      '--body', body,
                      '--label', ','.join(labels)
                  ], capture_output=True, text=True, check=True)

                  print(f"Created issue: {result.stdout.strip()}")
                  created_count += 1
              except subprocess.CalledProcessError as e:
                  print(f"Failed to create issue: {e.stderr}")

          print(f"\nCreated {created_count} issues")

          with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as f:
              f.write(f"\n\n## Issues Created\n\nCreated **{created_count}** new issues for untracked TODOs.\n")
          EOF


  trend-tracking:
    name: TODO Trend Tracking
    needs: scan-todos
    if: github.event_name != 'pull_request'
    runs-on: ubuntu-latest
    timeout-minutes: 10
    outputs:
      trend_direction: ${{ steps.trend.outputs.direction }}
      delta: ${{ steps.trend.outputs.delta }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v6.2.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download current scan results
        uses: actions/download-artifact@v4
        with:
          name: todo-scan-results

      - name: Compute TODO trend from git history
        id: trend
        run: |
          set -euo pipefail

          python3 << 'PYEOF'
          import json
          import subprocess
          import os
          from datetime import datetime, timezone, timedelta

          summary_file = os.environ['GITHUB_STEP_SUMMARY']

          # Current count
          current_count = 0
          if os.path.exists('todo-scan-results.json'):
              with open('todo-scan-results.json') as f:
                  current_count = json.load(f).get('total_count', 0)

          # Collect historical counts from the last 8 weekly commits that touched TODO.md
          result = subprocess.run(
              ['git', 'log', '--oneline', '--follow', '-20', '--', 'docs/status/TODO.md'],
              capture_output=True, text=True
          )
          commits = [line.split()[0] for line in result.stdout.strip().splitlines() if line]

          history = []
          for sha in commits[:8]:
              try:
                  blob = subprocess.run(
                      ['git', 'show', f'{sha}:docs/status/TODO.md'],
                      capture_output=True, text=True
                  )
                  if blob.returncode != 0:
                      continue
                  content = blob.stdout
                  # Parse total from "| **Total Items** | N |"
                  for line in content.splitlines():
                      if '**Total Items**' in line:
                          parts = line.split('|')
                          for p in parts:
                              p = p.strip().replace('**', '')
                              if p.isdigit():
                                  date_result = subprocess.run(
                                      ['git', 'log', '-1', '--format=%aI', sha],
                                      capture_output=True, text=True
                                  )
                                  date_str = date_result.stdout.strip()[:10]
                                  history.append({'date': date_str, 'count': int(p), 'sha': sha[:7]})
                                  break
                          break
              except Exception:
                  continue

          # Determine trend
          previous_count = history[0]['count'] if history else current_count
          delta = current_count - previous_count
          if delta > 0:
              direction = 'increasing'
              symbol = 'ðŸ“ˆ'
          elif delta < 0:
              direction = 'decreasing'
              symbol = 'ðŸ“‰'
          else:
              direction = 'stable'
              symbol = 'âž¡ï¸'

          with open(os.environ['GITHUB_OUTPUT'], 'a') as out:
              out.write(f'direction={direction}\n')
              out.write(f'delta={delta}\n')

          # Build summary
          lines = [
              '## TODO Trend Tracking',
              '',
              f'**Current count:** {current_count}',
              f'**Previous count:** {previous_count}',
              f'**Change:** {delta:+d} ({symbol} {direction})',
              '',
          ]

          if history:
              lines.extend([
                  '### Historical Counts',
                  '',
                  '| Date | Count | Commit |',
                  '|------|-------|--------|',
              ])
              for entry in history:
                  lines.append(f"| {entry['date']} | {entry['count']} | `{entry['sha']}` |")

              # Warn if trending upward over the last 4 data points
              recent = [h['count'] for h in history[:4]]
              if len(recent) >= 3 and all(recent[i] <= recent[i+1] for i in range(len(recent)-1)):
                  lines.extend([
                      '',
                      '> **Warning:** TODO count has been consistently increasing over recent scans. '
                      'Consider scheduling a cleanup sprint.',
                  ])
          else:
              lines.append('_No historical data available yet. Trends will appear after multiple runs._')

          with open(summary_file, 'a') as f:
              f.write('\n'.join(lines) + '\n')
          PYEOF

      - name: Upload trend data
        uses: actions/upload-artifact@v4
        with:
          name: todo-trend-data
          path: todo-scan-results.json
          retention-days: 90

  # â”€â”€â”€ Stale TODO Detection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  stale-todo-detection:
    name: Stale TODO Detection
    needs: scan-todos
    if: github.event_name != 'pull_request'
    runs-on: ubuntu-latest
    timeout-minutes: 15
    outputs:
      stale_count: ${{ steps.stale.outputs.stale_count }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v6.2.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download scan results
        uses: actions/download-artifact@v4
        with:
          name: todo-scan-results

      - name: Detect stale TODOs via git blame
        id: stale
        env:
          STALE_DAYS: ${{ github.event.inputs.stale_threshold_days || '90' }}
        run: |
          set -euo pipefail

          python3 << 'PYEOF'
          import json
          import subprocess
          import os
          from datetime import datetime, timezone, timedelta

          stale_days = int(os.environ.get('STALE_DAYS', '90'))
          cutoff = datetime.now(timezone.utc) - timedelta(days=stale_days)
          summary_file = os.environ['GITHUB_STEP_SUMMARY']

          if not os.path.exists('todo-scan-results.json'):
              with open(os.environ['GITHUB_OUTPUT'], 'a') as out:
                  out.write('stale_count=0\n')
              with open(summary_file, 'a') as f:
                  f.write('## Stale TODO Detection\n\nNo scan results available.\n')
              raise SystemExit(0)

          with open('todo-scan-results.json') as f:
              data = json.load(f)

          stale_todos = []

          for todo in data.get('todos', []):
              file_path = todo.get('file', '')
              line_num = todo.get('line', 0)

              if not file_path or not os.path.exists(file_path):
                  continue

              try:
                  result = subprocess.run(
                      ['git', 'blame', '-L', f'{line_num},{line_num}', '--porcelain', file_path],
                      capture_output=True, text=True, timeout=10
                  )
                  if result.returncode != 0:
                      continue

                  author_time = None
                  author = None
                  for blame_line in result.stdout.splitlines():
                      if blame_line.startswith('author-time '):
                          timestamp = int(blame_line.split(' ', 1)[1])
                          author_time = datetime.fromtimestamp(timestamp, tz=timezone.utc)
                      elif blame_line.startswith('author '):
                          author = blame_line.split(' ', 1)[1]

                  if author_time and author_time < cutoff:
                      age_days = (datetime.now(timezone.utc) - author_time).days
                      stale_todos.append({
                          'type': todo.get('type', 'TODO'),
                          'text': todo.get('text', '')[:80],
                          'file': file_path,
                          'line': line_num,
                          'author': author or 'unknown',
                          'date': author_time.strftime('%Y-%m-%d'),
                          'age_days': age_days,
                          'priority': todo.get('priority', 'normal'),
                      })
              except (subprocess.TimeoutExpired, Exception):
                  continue

          stale_todos.sort(key=lambda x: -x['age_days'])

          with open(os.environ['GITHUB_OUTPUT'], 'a') as out:
              out.write(f'stale_count={len(stale_todos)}\n')

          # Save stale list for other jobs
          with open('stale-todos.json', 'w') as f:
              json.dump(stale_todos, f, indent=2)

          lines = [
              '## Stale TODO Detection',
              '',
              f'**Threshold:** {stale_days} days',
              f'**Stale TODOs found:** {len(stale_todos)}',
              '',
          ]

          if stale_todos:
              lines.extend([
                  '### Oldest TODOs',
                  '',
                  '| Age (days) | Type | File | Author | Description |',
                  '|------------|------|------|--------|-------------|',
              ])
              for todo in stale_todos[:20]:
                  desc = todo["text"][:50] + ("..." if len(todo["text"]) > 50 else "")
                  lines.append(
                      f'| {todo["age_days"]} | `{todo["type"]}` '
                      f'| `{todo["file"]}:{todo["line"]}` '
                      f'| {todo["author"]} | {desc} |'
                  )

              if len(stale_todos) > 20:
                  lines.append(f'| ... | ... | ... | ... | _and {len(stale_todos) - 20} more_ |')

              # Age distribution
              buckets = {'90-180 days': 0, '180-365 days': 0, '1+ years': 0}
              for t in stale_todos:
                  if t['age_days'] < 180:
                      buckets['90-180 days'] += 1
                  elif t['age_days'] < 365:
                      buckets['180-365 days'] += 1
                  else:
                      buckets['1+ years'] += 1

              lines.extend([
                  '',
                  '### Age Distribution',
                  '',
                  '| Range | Count |',
                  '|-------|-------|',
              ])
              for label, count in buckets.items():
                  if count > 0:
                      lines.append(f'| {label} | {count} |')
          else:
              lines.append('No TODOs older than the threshold were found.')

          with open(summary_file, 'a') as f:
              f.write('\n'.join(lines) + '\n')
          PYEOF

      - name: Upload stale TODO data
        uses: actions/upload-artifact@v4
        with:
          name: stale-todos
          path: stale-todos.json
          retention-days: 30

  # â”€â”€â”€ Resolved Issue Audit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  resolved-issue-audit:
    name: Resolved Issue Audit
    needs: scan-todos
    if: github.event_name != 'pull_request'
    runs-on: ubuntu-latest
    timeout-minutes: 10
    outputs:
      orphaned_count: ${{ steps.audit.outputs.orphaned_count }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v6.2.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download scan results
        uses: actions/download-artifact@v4
        with:
          name: todo-scan-results

      - name: Audit TODOs with closed issues
        id: audit
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euo pipefail

          python3 << 'PYEOF'
          import json
          import subprocess
          import os

          summary_file = os.environ['GITHUB_STEP_SUMMARY']

          if not os.path.exists('todo-scan-results.json'):
              with open(os.environ['GITHUB_OUTPUT'], 'a') as out:
                  out.write('orphaned_count=0\n')
              with open(summary_file, 'a') as f:
                  f.write('## Resolved Issue Audit\n\nNo scan results available.\n')
              raise SystemExit(0)

          with open('todo-scan-results.json') as f:
              data = json.load(f)

          # Collect all referenced issue numbers
          issue_refs = set()
          for todo in data.get('todos', []):
              for ref in todo.get('issue_refs', []):
                  try:
                      issue_refs.add(int(ref))
                  except (ValueError, TypeError):
                      pass

          if not issue_refs:
              with open(os.environ['GITHUB_OUTPUT'], 'a') as out:
                  out.write('orphaned_count=0\n')
              with open(summary_file, 'a') as f:
                  f.write('## Resolved Issue Audit\n\nNo TODOs reference GitHub issues. Nothing to audit.\n')
              raise SystemExit(0)

          # Check status of each referenced issue
          closed_issues = {}
          for issue_num in sorted(issue_refs):
              try:
                  result = subprocess.run(
                      ['gh', 'issue', 'view', str(issue_num), '--json', 'state,title,closedAt'],
                      capture_output=True, text=True, timeout=15
                  )
                  if result.returncode == 0:
                      issue_data = json.loads(result.stdout)
                      if issue_data.get('state') == 'CLOSED':
                          closed_issues[issue_num] = {
                              'title': issue_data.get('title', ''),
                              'closed_at': issue_data.get('closedAt', '')[:10],
                          }
              except (subprocess.TimeoutExpired, json.JSONDecodeError, Exception):
                  continue

          # Find TODOs that reference closed issues
          orphaned = []
          for todo in data.get('todos', []):
              for ref in todo.get('issue_refs', []):
                  try:
                      ref_num = int(ref)
                  except (ValueError, TypeError):
                      continue
                  if ref_num in closed_issues:
                      orphaned.append({
                          'type': todo.get('type', 'TODO'),
                          'text': todo.get('text', '')[:60],
                          'file': todo.get('file', ''),
                          'line': todo.get('line', 0),
                          'issue': ref_num,
                          'issue_title': closed_issues[ref_num]['title'][:50],
                          'closed_at': closed_issues[ref_num]['closed_at'],
                      })

          with open(os.environ['GITHUB_OUTPUT'], 'a') as out:
              out.write(f'orphaned_count={len(orphaned)}\n')

          with open('orphaned-todos.json', 'w') as f:
              json.dump(orphaned, f, indent=2)

          lines = [
              '## Resolved Issue Audit',
              '',
              f'**Issues referenced by TODOs:** {len(issue_refs)}',
              f'**Closed issues still in code:** {len(closed_issues)}',
              f'**Orphaned TODO comments:** {len(orphaned)}',
              '',
          ]

          if orphaned:
              lines.extend([
                  '### Orphaned TODOs (linked issue is closed)',
                  '',
                  'These TODO comments reference issues that have been closed. '
                  'The TODO may be stale and eligible for removal.',
                  '',
                  '| File | Type | Issue | Closed | Description |',
                  '|------|------|-------|--------|-------------|',
              ])
              for item in orphaned:
                  lines.append(
                      f'| `{item["file"]}:{item["line"]}` '
                      f'| `{item["type"]}` '
                      f'| #{item["issue"]} '
                      f'| {item["closed_at"]} '
                      f'| {item["text"]} |'
                  )
              lines.extend([
                  '',
                  '> **Action:** Review these TODOs and remove them if the linked issue '
                  'has been fully resolved, or re-open the issue if work remains.',
              ])
          else:
              lines.append('All TODO-linked issues are still open. No orphaned TODOs found.')

          with open(summary_file, 'a') as f:
              f.write('\n'.join(lines) + '\n')
          PYEOF

      - name: Upload orphaned TODO data
        uses: actions/upload-artifact@v4
        with:
          name: orphaned-todos
          path: orphaned-todos.json
          retention-days: 30

  # â”€â”€â”€ PR TODO Diff â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  pr-todo-diff:
    name: PR TODO Diff
    if: github.event_name == 'pull_request'
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout PR
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v6.2.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Detect new TODOs in PR diff
        id: diff
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euo pipefail

          python3 << 'PYEOF'
          import re
          import subprocess
          import json
          import os

          summary_file = os.environ['GITHUB_STEP_SUMMARY']

          # Get the diff against the base branch
          base_ref = os.environ.get('GITHUB_BASE_REF', 'main')
          result = subprocess.run(
              ['git', 'diff', f'origin/{base_ref}...HEAD', '--unified=0', '--diff-filter=AM'],
              capture_output=True, text=True
          )

          if result.returncode != 0:
              with open(os.environ['GITHUB_OUTPUT'], 'a') as out:
                  out.write('new_todo_count=0\n')
                  out.write('comment_body=\n')
              raise SystemExit(0)

          diff_text = result.stdout

          # Parse added lines from the diff
          TODO_TYPES = ['TODO', 'FIXME', 'HACK', 'BUG', 'XXX', 'PERF', 'OPTIMIZE', 'REFACTOR', 'NOTE']
          types_pattern = '|'.join(TODO_TYPES)
          todo_regex = re.compile(rf'\b({types_pattern})[\s:]+(.+)', re.IGNORECASE)

          current_file = None
          new_todos = []

          for line in diff_text.splitlines():
              if line.startswith('diff --git'):
                  # Extract file path: diff --git a/path b/path
                  parts = line.split(' b/')
                  current_file = parts[-1] if len(parts) > 1 else None
              elif line.startswith('+') and not line.startswith('+++'):
                  added_content = line[1:]
                  match = todo_regex.search(added_content)
                  if match:
                      new_todos.append({
                          'file': current_file or 'unknown',
                          'type': match.group(1).upper(),
                          'text': match.group(2).strip()[:120],
                      })

          with open(os.environ['GITHUB_OUTPUT'], 'a') as out:
              out.write(f'new_todo_count={len(new_todos)}\n')

          # Build PR comment
          if new_todos:
              comment_lines = [
                  '## ðŸ“‹ New TODO Comments Detected',
                  '',
                  f'This PR introduces **{len(new_todos)}** new TODO comment(s):',
                  '',
                  '| Type | File | Description |',
                  '|------|------|-------------|',
              ]
              for t in new_todos:
                  desc = t["text"][:80] + ("..." if len(t["text"]) > 80 else "")
                  comment_lines.append(f'| `{t["type"]}` | `{t["file"]}` | {desc} |')

              comment_lines.extend([
                  '',
                  '<details><summary>Guidelines</summary>',
                  '',
                  '- Link TODOs to GitHub issues: `// TODO: Track with issue #123`',
                  '- Use `FIXME` for bugs, `HACK` for temporary workarounds',
                  '- Consider creating an issue for non-trivial TODOs',
                  '',
                  '</details>',
                  '',
                  '_Reported by TODO Automation workflow._',
              ])

              comment_body = '\n'.join(comment_lines)

              # Write comment body to file for the next step
              with open('pr-comment.md', 'w') as f:
                  f.write(comment_body)
          else:
              with open('pr-comment.md', 'w') as f:
                  f.write('')

          # Step summary
          lines = [
              '## PR TODO Diff',
              '',
              f'**New TODOs in this PR:** {len(new_todos)}',
              '',
          ]
          if new_todos:
              lines.extend([
                  '| Type | File | Description |',
                  '|------|------|-------------|',
              ])
              for t in new_todos:
                  desc = t["text"][:80] + ("..." if len(t["text"]) > 80 else "")
                  lines.append(f'| `{t["type"]}` | `{t["file"]}` | {desc} |')
          else:
              lines.append('No new TODO comments introduced in this PR.')

          with open(summary_file, 'a') as f:
              f.write('\n'.join(lines) + '\n')
          PYEOF

      - name: Comment on PR with new TODOs
        if: hashFiles('pr-comment.md') != '' && steps.diff.outputs.new_todo_count != '0'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Check for existing TODO comment to avoid duplicates
          EXISTING=$(gh pr view "${{ github.event.pull_request.number }}" \
            --json comments --jq '.comments[].body' 2>/dev/null | \
            grep -c "New TODO Comments Detected" || true)

          if [ "$EXISTING" -gt 0 ]; then
            echo "TODO comment already exists on this PR, skipping."
          else
            gh pr comment "${{ github.event.pull_request.number }}" \
              --body-file pr-comment.md
          fi

  # â”€â”€â”€ Duplicate Detection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  duplicate-detection:
    name: Duplicate TODO Detection
    needs: scan-todos
    if: github.event_name != 'pull_request'
    runs-on: ubuntu-latest
    timeout-minutes: 10
    outputs:
      duplicate_groups: ${{ steps.dupes.outputs.duplicate_groups }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v6.2.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download scan results
        uses: actions/download-artifact@v4
        with:
          name: todo-scan-results

      - name: Detect duplicate TODOs
        id: dupes
        run: |
          set -euo pipefail

          python3 << 'PYEOF'
          import json
          import os
          import re
          from collections import defaultdict

          summary_file = os.environ['GITHUB_STEP_SUMMARY']

          if not os.path.exists('todo-scan-results.json'):
              with open(os.environ['GITHUB_OUTPUT'], 'a') as out:
                  out.write('duplicate_groups=0\n')
              with open(summary_file, 'a') as f:
                  f.write('## Duplicate Detection\n\nNo scan results available.\n')
              raise SystemExit(0)

          with open('todo-scan-results.json') as f:
              data = json.load(f)

          todos = data.get('todos', [])

          def normalize(text):
              """Normalize text for comparison: lowercase, strip punctuation, collapse whitespace."""
              text = text.lower().strip()
              text = re.sub(r'[^\w\s]', '', text)
              text = re.sub(r'\s+', ' ', text)
              # Remove common prefixes like issue references
              text = re.sub(r'track with issue \d+', '', text)
              text = re.sub(r'issue \d+', '', text)
              return text.strip()

          def tokenize(text):
              """Create a set of significant tokens for comparison."""
              stop_words = {'the', 'a', 'an', 'is', 'are', 'was', 'be', 'to', 'of', 'and',
                           'in', 'for', 'on', 'with', 'this', 'that', 'it', 'not', 'but',
                           'we', 'need', 'should', 'could', 'would', 'will', 'can'}
              words = normalize(text).split()
              return {w for w in words if w not in stop_words and len(w) > 2}

          def similarity(tokens_a, tokens_b):
              """Jaccard similarity between two token sets."""
              if not tokens_a or not tokens_b:
                  return 0.0
              intersection = tokens_a & tokens_b
              union = tokens_a | tokens_b
              return len(intersection) / len(union) if union else 0.0

          # Index all TODOs by their tokens
          indexed = []
          for i, todo in enumerate(todos):
              tokens = tokenize(todo.get('text', ''))
              if len(tokens) >= 2:  # skip very short texts
                  indexed.append((i, tokens, todo))

          # Find duplicate groups (similarity >= 0.6)
          THRESHOLD = 0.6
          visited = set()
          groups = []

          for i, (idx_a, tokens_a, todo_a) in enumerate(indexed):
              if idx_a in visited:
                  continue
              group = [todo_a]
              for j, (idx_b, tokens_b, todo_b) in enumerate(indexed):
                  if idx_b in visited or idx_a == idx_b:
                      continue
                  if similarity(tokens_a, tokens_b) >= THRESHOLD:
                      group.append(todo_b)
                      visited.add(idx_b)
              if len(group) > 1:
                  visited.add(idx_a)
                  groups.append(group)

          with open(os.environ['GITHUB_OUTPUT'], 'a') as out:
              out.write(f'duplicate_groups={len(groups)}\n')

          with open('duplicate-todos.json', 'w') as f:
              json.dump([
                  [{'type': t.get('type'), 'text': t.get('text', '')[:80],
                    'file': t.get('file'), 'line': t.get('line')} for t in g]
                  for g in groups
              ], f, indent=2)

          lines = [
              '## Duplicate TODO Detection',
              '',
              f'**Total TODOs analyzed:** {len(indexed)}',
              f'**Duplicate groups found:** {len(groups)}',
              f'**Similarity threshold:** {THRESHOLD:.0%}',
              '',
          ]

          if groups:
              lines.append('### Duplicate Groups')
              lines.append('')
              for gi, group in enumerate(groups[:10], 1):
                  lines.append(f'**Group {gi}** ({len(group)} items):')
                  for item in group:
                      text = item.get("text", "")[:60]
                      lines.append(
                          f'- `{item.get("type", "TODO")}` in `{item.get("file", "?")}:{item.get("line", 0)}` â€” {text}'
                      )
                  lines.append('')

              if len(groups) > 10:
                  lines.append(f'_...and {len(groups) - 10} more group(s)._')

              lines.extend([
                  '',
                  '> **Recommendation:** Consider consolidating duplicate TODOs into '
                  'a single tracked issue to reduce noise.',
              ])
          else:
              lines.append('No duplicate or highly similar TODOs detected.')

          with open(summary_file, 'a') as f:
              f.write('\n'.join(lines) + '\n')
          PYEOF

      - name: Upload duplicate data
        uses: actions/upload-artifact@v4
        with:
          name: duplicate-todos
          path: duplicate-todos.json
          retention-days: 30

  # â”€â”€â”€ Hotspot Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  hotspot-analysis:
    name: TODO Hotspot Analysis
    needs: scan-todos
    if: github.event_name != 'pull_request'
    runs-on: ubuntu-latest
    timeout-minutes: 10
    outputs:
      top_hotspot: ${{ steps.hotspots.outputs.top_hotspot }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v6.2.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download scan results
        uses: actions/download-artifact@v4
        with:
          name: todo-scan-results

      - name: Analyze TODO hotspots
        id: hotspots
        run: |
          set -euo pipefail

          python3 << 'PYEOF'
          import json
          import os
          import subprocess
          from collections import defaultdict
          from pathlib import Path

          summary_file = os.environ['GITHUB_STEP_SUMMARY']

          if not os.path.exists('todo-scan-results.json'):
              with open(os.environ['GITHUB_OUTPUT'], 'a') as out:
                  out.write('top_hotspot=none\n')
              with open(summary_file, 'a') as f:
                  f.write('## TODO Hotspot Analysis\n\nNo scan results available.\n')
              raise SystemExit(0)

          with open('todo-scan-results.json') as f:
              data = json.load(f)

          todos = data.get('todos', [])

          # === By file ===
          by_file = defaultdict(list)
          for todo in todos:
              by_file[todo.get('file', 'unknown')].append(todo)

          # Calculate density: TODOs / file lines
          file_density = []
          for file_path, items in by_file.items():
              try:
                  if os.path.exists(file_path):
                      line_count = sum(1 for _ in open(file_path, encoding='utf-8', errors='replace'))
                      density = len(items) / max(line_count, 1)
                  else:
                      line_count = 0
                      density = 0
              except Exception:
                  line_count = 0
                  density = 0

              file_density.append({
                  'file': file_path,
                  'count': len(items),
                  'lines': line_count,
                  'density': density,
                  'types': list(set(t.get('type', 'TODO') for t in items)),
                  'high_priority': sum(1 for t in items if t.get('priority') == 'high'),
              })

          # Sort by count desc, then density desc
          file_density.sort(key=lambda x: (-x['count'], -x['density']))

          # === By directory (2-level deep) ===
          by_dir = defaultdict(lambda: {'count': 0, 'files': set(), 'types': defaultdict(int)})
          for todo in todos:
              parts = Path(todo.get('file', '')).parts
              dir_key = '/'.join(parts[:3]) if len(parts) >= 3 else '/'.join(parts[:2]) if len(parts) >= 2 else 'root'
              by_dir[dir_key]['count'] += 1
              by_dir[dir_key]['files'].add(todo.get('file', ''))
              by_dir[dir_key]['types'][todo.get('type', 'TODO')] += 1

          dir_hotspots = sorted(by_dir.items(), key=lambda x: -x[1]['count'])

          top_hotspot = file_density[0]['file'] if file_density else 'none'
          with open(os.environ['GITHUB_OUTPUT'], 'a') as out:
              out.write(f'top_hotspot={top_hotspot}\n')

          lines = [
              '## TODO Hotspot Analysis',
              '',
              f'**Files with TODOs:** {len(by_file)}',
              f'**Directories with TODOs:** {len(by_dir)}',
              '',
          ]

          # Top files by count
          if file_density:
              lines.extend([
                  '### Top Files by TODO Count',
                  '',
                  '| File | TODOs | Lines | Density | High Priority | Types |',
                  '|------|-------|-------|---------|---------------|-------|',
              ])
              for fd in file_density[:15]:
                  density_pct = f'{fd["density"]*100:.1f}%'
                  types_str = ', '.join(fd['types'])
                  lines.append(
                      f'| `{fd["file"]}` | {fd["count"]} | {fd["lines"]} '
                      f'| {density_pct} | {fd["high_priority"]} | {types_str} |'
                  )
              lines.append('')

          # Top directories
          if dir_hotspots:
              lines.extend([
                  '### Top Directories by TODO Count',
                  '',
                  '| Directory | TODOs | Files | Dominant Type |',
                  '|-----------|-------|-------|---------------|',
              ])
              for dir_name, info in dir_hotspots[:10]:
                  dominant = max(info['types'].items(), key=lambda x: x[1])[0] if info['types'] else 'N/A'
                  lines.append(
                      f'| `{dir_name}/` | {info["count"]} | {len(info["files"])} | `{dominant}` |'
                  )
              lines.append('')

          # High-density warning
          high_density = [f for f in file_density if f['density'] > 0.05 and f['count'] >= 3]
          if high_density:
              lines.extend([
                  '### High-Density Files (>5% of lines are TODOs)',
                  '',
              ])
              for fd in high_density[:5]:
                  density_pct = f'{fd["density"]*100:.1f}%'
                  lines.append(f'- `{fd["file"]}`: {fd["count"]} TODOs in {fd["lines"]} lines ({density_pct})')
              lines.extend([
                  '',
                  '> **Action:** These files have a high concentration of TODOs and may '
                  'benefit from focused refactoring attention.',
              ])

          with open(summary_file, 'a') as f:
              f.write('\n'.join(lines) + '\n')
          PYEOF

  # â”€â”€â”€ Create Issues â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  # â”€â”€â”€ Report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  report:
    name: Documentation Automation Report
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [detect-changes, validate-docs, regenerate-docs, scan-todos, create-todo-issues, trend-tracking, stale-todo-detection, resolved-issue-audit, pr-todo-diff, duplicate-detection, hotspot-analysis]
    if: github.event_name != 'issues' && always()
    steps:
      - name: Publish summary
        run: |
          echo "# Documentation Automation Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Trigger:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Branch:** ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Change Detection" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Area | Changed |" >> $GITHUB_STEP_SUMMARY
          echo "|------|---------|" >> $GITHUB_STEP_SUMMARY
          echo "| Docs | ${{ needs.detect-changes.outputs.docs_changed }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Workflows | ${{ needs.detect-changes.outputs.workflow_changed }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Structure | ${{ needs.detect-changes.outputs.structure_changed }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Providers | ${{ needs.detect-changes.outputs.providers_changed }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Interfaces | ${{ needs.detect-changes.outputs.interfaces_changed }} |" >> $GITHUB_STEP_SUMMARY
          echo "| ADRs | ${{ needs.detect-changes.outputs.adrs_changed }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Configuration | ${{ needs.detect-changes.outputs.config_changed }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Microservices | ${{ needs.detect-changes.outputs.microservices_changed }} |" >> $GITHUB_STEP_SUMMARY
          echo "| AI files | ${{ needs.detect-changes.outputs.ai_files_changed }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Source code | ${{ needs.detect-changes.outputs.src_changed }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Job Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Job | Result |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Validate docs | ${{ needs.validate-docs.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Regenerate docs | ${{ needs.regenerate-docs.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Scan TODOs | ${{ needs.scan-todos.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Create TODO issues | ${{ needs.create-todo-issues.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Generated Changes | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|-------------------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| has_changes | ${{ needs.regenerate-docs.outputs.has_changes || 'false' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| changed_files | ${{ needs.regenerate-docs.outputs.changed_files || 'none' }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## TODO Scan" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Total TODOs | ${{ needs.scan-todos.outputs.total_count || 'N/A' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Untracked TODOs | ${{ needs.scan-todos.outputs.new_todos || 'N/A' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| TODO doc updated | ${{ needs.scan-todos.outputs.has_changes || 'N/A' }} |" >> $GITHUB_STEP_SUMMARY
