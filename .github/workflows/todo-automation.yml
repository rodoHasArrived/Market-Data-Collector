# GitHub Action: TODO Automation and Documentation
# This workflow scans the codebase for TODO/FIXME/HACK/NOTE comments,
# generates documentation, and optionally creates GitHub issues for tracking.
#
# Triggers:
#   - Push to main (src/** or tests/**)
#   - Pull requests to main (comments on new TODOs)
#   - Weekly schedule (Monday 4 AM UTC)
#   - Manual dispatch with options
#
# Features:
#   - Scans source files for TODO markers
#   - Generates docs/status/TODO.md with all tracked items
#   - Links existing GitHub issues to TODO comments
#   - Can create issues for untracked TODOs (manual trigger only)
#   - Categorizes by priority, type, and file location
#   - Trend tracking: compares TODO counts across runs
#   - Stale TODO detection: finds TODOs older than 90 days via git blame
#   - Resolved-issue audit: flags TODOs whose linked issues are closed
#   - PR diff: comments on PRs that introduce new TODOs
#   - Duplicate detection: finds similar/duplicate TODO comments
#   - Hotspot analysis: identifies files with highest TODO density

name: TODO Automation

on:
  push:
    branches: ["main"]
    paths:
      - 'src/**'
      - 'tests/**'
      - 'benchmarks/**'
      - 'build/**/*.py'
  pull_request:
    branches: ["main"]
    paths:
      - 'src/**'
      - 'tests/**'
      - 'benchmarks/**'
  schedule:
    # Run weekly on Monday at 4 AM UTC
    - cron: '0 4 * * 1'
  workflow_dispatch:
    inputs:
      create_issues:
        description: 'Create GitHub issues for untracked TODOs'
        type: boolean
        default: false
        required: false
      dry_run:
        description: 'Dry run - generate report without committing'
        type: boolean
        default: false
        required: false
      include_notes:
        description: 'Include NOTE comments in documentation'
        type: boolean
        default: true
        required: false
      use_copilot:
        description: 'Use Copilot/AI to generate TODO triage recommendations'
        type: boolean
        default: true
        required: false
      stale_threshold_days:
        description: 'Days before a TODO is considered stale'
        type: number
        default: 90
        required: false
      close_resolved_issues:
        description: 'Auto-comment on TODOs whose linked issues are closed'
        type: boolean
        default: false
        required: false

permissions:
  contents: write
  issues: write
  pull-requests: write
  models: read

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: '3.11'

jobs:
  # â”€â”€â”€ Core Scan â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  scan-todos:
    name: Scan and Document TODOs
    runs-on: ubuntu-latest
    timeout-minutes: 15
    outputs:
      total_count: ${{ steps.scan.outputs.total_count }}
      new_todos: ${{ steps.scan.outputs.new_todos }}
      has_changes: ${{ steps.check-changes.outputs.has_changes }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Scan codebase for TODOs
        id: scan
        run: |
          python3 build/scripts/docs/scan-todos.py \
            --output docs/status/TODO.md \
            --format markdown \
            --include-notes ${{ github.event.inputs.include_notes || 'true' }} \
            --json-output todo-scan-results.json

          # Extract metrics for workflow outputs
          if [ -f todo-scan-results.json ]; then
            TOTAL=$(python3 -c "import json; data=json.load(open('todo-scan-results.json')); print(data.get('total_count', 0))")
            NEW=$(python3 -c "import json; data=json.load(open('todo-scan-results.json')); print(len([t for t in data.get('todos', []) if not t.get('has_issue', False)]))")
            SCAN_JSON=$(python3 -c "import json; data=json.load(open('todo-scan-results.json')); print(json.dumps(data, separators=(',', ':')))")
            echo "total_count=$TOTAL" >> $GITHUB_OUTPUT
            echo "new_todos=$NEW" >> $GITHUB_OUTPUT
            echo "scan_json=$SCAN_JSON" >> $GITHUB_OUTPUT
          else
            echo "total_count=0" >> $GITHUB_OUTPUT
            echo "new_todos=0" >> $GITHUB_OUTPUT
            echo 'scan_json={"total_count":0,"todos":[]}' >> $GITHUB_OUTPUT
          fi


      - name: Generate Copilot triage recommendations
        id: copilot
        if: github.event.inputs.use_copilot != 'false'
        continue-on-error: true
        uses: actions/ai-inference@v1
        with:
          model: openai/gpt-4.1-mini
          max-tokens: 1800
          prompt: |
            You are an engineering assistant helping triage TODO comments in a repository.
            Produce a concise markdown report with these sections:
            1) Highest-priority TODOs (max 5)
            2) Recommended issue labels
            3) Immediate risk items
            4) Suggested batching plan

            Keep the output under 300 words.
            Use this scan JSON:
            ${{ steps.scan.outputs.scan_json }}

      - name: Build fallback triage report
        if: always()
        run: |
          set -euo pipefail

          python3 << 'EOF'
          import json
          from pathlib import Path

          scan_path = Path('todo-scan-results.json')
          report_path = Path('todo-triage.md')

          if not scan_path.exists():
              report_path.write_text('# TODO Triage\n\nNo scan results were found.\n', encoding='utf-8')
              raise SystemExit(0)

          data = json.loads(scan_path.read_text(encoding='utf-8'))
          todos = data.get('todos', [])
          untracked = [t for t in todos if not t.get('has_issue', False)]

          lines = [
              '# TODO Triage Recommendations',
              '',
              '_Generated by TODO Automation workflow. Copilot output is appended when available; otherwise this deterministic fallback is produced._',
              '',
              f"- Total TODOs: **{data.get('total_count', 0)}**",
              f"- Untracked TODOs: **{len(untracked)}**",
              '',
              '## Highest-priority TODOs (fallback)',
              ''
          ]

          for i, todo in enumerate(untracked[:5], 1):
              todo_type = todo.get('type', 'TODO')
              text = todo.get('text', 'No description')
              file_path = todo.get('file', 'unknown')
              line = todo.get('line', 0)
              lines.append(f"{i}. **[{todo_type}]** `{file_path}:{line}` â€” {text}")

          if not untracked:
              lines.append('- No untracked TODO items found.')

          lines.extend([
              '',
              '## Suggested labels',
              '',
              '- `todo-automation` for all imported TODO items',
              '- `bug` for FIXME items',
              '- `tech-debt` for HACK items',
              '',
              '## Suggested batching plan',
              '',
              '1. Resolve untracked FIXME items first.',
              '2. Group TODO/HACK items by directory owner.',
              '3. Convert remaining high-value TODOs into tracked issues.'
          ])

          report_path.write_text('\n'.join(lines) + '\n', encoding='utf-8')
          EOF

      - name: Append Copilot triage output to report
        if: always() && steps.copilot.outputs.response != ''
        run: |
          {
            echo ""
            echo "## Copilot Recommendations"
            echo ""
            echo "${{ steps.copilot.outputs.response }}"
          } >> todo-triage.md
      - name: Check for changes
        id: check-changes
        if: github.event_name != 'pull_request'
        run: |
          git add docs/status/TODO.md
          if git diff --staged --quiet; then
            echo "has_changes=false" >> $GITHUB_OUTPUT
            echo "No changes to TODO documentation"
          else
            echo "has_changes=true" >> $GITHUB_OUTPUT
            echo "Changes detected in TODO documentation"
          fi

      - name: Generate summary
        run: |
          echo "## TODO Scan Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f todo-scan-results.json ]; then
            python3 << 'EOF'
          import json
          import os

          with open('todo-scan-results.json', 'r') as f:
              data = json.load(f)

          summary = []
          summary.append(f"**Total TODOs:** {data.get('total_count', 0)}")
          summary.append(f"**Linked to Issues:** {len([t for t in data.get('todos', []) if t.get('has_issue', False)])}")
          summary.append(f"**Untracked:** {len([t for t in data.get('todos', []) if not t.get('has_issue', False)])}")
          summary.append("")

          # By type
          by_type = {}
          for todo in data.get('todos', []):
              t = todo.get('type', 'TODO')
              by_type[t] = by_type.get(t, 0) + 1

          if by_type:
              summary.append("### By Type")
              summary.append("| Type | Count |")
              summary.append("|------|-------|")
              for t, count in sorted(by_type.items()):
                  summary.append(f"| {t} | {count} |")
              summary.append("")

          # By directory
          by_dir = {}
          for todo in data.get('todos', []):
              path = todo.get('file', '')
              parts = path.split('/')
              if len(parts) > 1:
                  dir_name = parts[0] if parts[0] != '.' else parts[1] if len(parts) > 1 else 'root'
              else:
                  dir_name = 'root'
              by_dir[dir_name] = by_dir.get(dir_name, 0) + 1

          if by_dir:
              summary.append("### By Directory")
              summary.append("| Directory | Count |")
              summary.append("|-----------|-------|")
              for d, count in sorted(by_dir.items(), key=lambda x: -x[1]):
                  summary.append(f"| `{d}/` | {count} |")

          with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as f:
              f.write('\n'.join(summary))
          EOF
          else
            echo "No TODO scan results found." >> $GITHUB_STEP_SUMMARY
          fi

          if [ -f todo-triage.md ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Triage Recommendations" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            cat todo-triage.md >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload scan results
        uses: actions/upload-artifact@v4
        with:
          name: todo-scan-results
          path: |
            todo-scan-results.json
            docs/status/TODO.md
            todo-triage.md
          retention-days: 30

      - name: Commit changes
        if: |
          steps.check-changes.outputs.has_changes == 'true' &&
          github.event.inputs.dry_run != 'true' &&
          github.event_name != 'pull_request'
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git add docs/status/TODO.md
          git commit -m "docs: update TODO documentation [skip ci]

          Auto-generated TODO tracking documentation.

          - Total TODOs: ${{ steps.scan.outputs.total_count }}
          - Untracked: ${{ steps.scan.outputs.new_todos }}

          Generated by TODO Automation workflow."

          git push

  # â”€â”€â”€ Trend Tracking â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  trend-tracking:
    name: TODO Trend Tracking
    needs: scan-todos
    if: github.event_name != 'pull_request'
    runs-on: ubuntu-latest
    timeout-minutes: 10
    outputs:
      trend_direction: ${{ steps.trend.outputs.direction }}
      delta: ${{ steps.trend.outputs.delta }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download current scan results
        uses: actions/download-artifact@v4
        with:
          name: todo-scan-results

      - name: Compute TODO trend from git history
        id: trend
        run: |
          set -euo pipefail

          python3 << 'PYEOF'
          import json
          import subprocess
          import os
          from datetime import datetime, timezone, timedelta

          summary_file = os.environ['GITHUB_STEP_SUMMARY']

          # Current count
          current_count = 0
          if os.path.exists('todo-scan-results.json'):
              with open('todo-scan-results.json') as f:
                  current_count = json.load(f).get('total_count', 0)

          # Collect historical counts from the last 8 weekly commits that touched TODO.md
          result = subprocess.run(
              ['git', 'log', '--oneline', '--follow', '-20', '--', 'docs/status/TODO.md'],
              capture_output=True, text=True
          )
          commits = [line.split()[0] for line in result.stdout.strip().splitlines() if line]

          history = []
          for sha in commits[:8]:
              try:
                  blob = subprocess.run(
                      ['git', 'show', f'{sha}:docs/status/TODO.md'],
                      capture_output=True, text=True
                  )
                  if blob.returncode != 0:
                      continue
                  content = blob.stdout
                  # Parse total from "| **Total Items** | N |"
                  for line in content.splitlines():
                      if '**Total Items**' in line:
                          parts = line.split('|')
                          for p in parts:
                              p = p.strip().replace('**', '')
                              if p.isdigit():
                                  date_result = subprocess.run(
                                      ['git', 'log', '-1', '--format=%aI', sha],
                                      capture_output=True, text=True
                                  )
                                  date_str = date_result.stdout.strip()[:10]
                                  history.append({'date': date_str, 'count': int(p), 'sha': sha[:7]})
                                  break
                          break
              except Exception:
                  continue

          # Determine trend
          previous_count = history[0]['count'] if history else current_count
          delta = current_count - previous_count
          if delta > 0:
              direction = 'increasing'
              symbol = 'ðŸ“ˆ'
          elif delta < 0:
              direction = 'decreasing'
              symbol = 'ðŸ“‰'
          else:
              direction = 'stable'
              symbol = 'âž¡ï¸'

          with open(os.environ['GITHUB_OUTPUT'], 'a') as out:
              out.write(f'direction={direction}\n')
              out.write(f'delta={delta}\n')

          # Build summary
          lines = [
              '## TODO Trend Tracking',
              '',
              f'**Current count:** {current_count}',
              f'**Previous count:** {previous_count}',
              f'**Change:** {delta:+d} ({symbol} {direction})',
              '',
          ]

          if history:
              lines.extend([
                  '### Historical Counts',
                  '',
                  '| Date | Count | Commit |',
                  '|------|-------|--------|',
              ])
              for entry in history:
                  lines.append(f"| {entry['date']} | {entry['count']} | `{entry['sha']}` |")

              # Warn if trending upward over the last 4 data points
              recent = [h['count'] for h in history[:4]]
              if len(recent) >= 3 and all(recent[i] <= recent[i+1] for i in range(len(recent)-1)):
                  lines.extend([
                      '',
                      '> **Warning:** TODO count has been consistently increasing over recent scans. '
                      'Consider scheduling a cleanup sprint.',
                  ])
          else:
              lines.append('_No historical data available yet. Trends will appear after multiple runs._')

          with open(summary_file, 'a') as f:
              f.write('\n'.join(lines) + '\n')
          PYEOF

      - name: Upload trend data
        uses: actions/upload-artifact@v4
        with:
          name: todo-trend-data
          path: todo-scan-results.json
          retention-days: 90

  # â”€â”€â”€ Stale TODO Detection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  stale-todo-detection:
    name: Stale TODO Detection
    needs: scan-todos
    if: github.event_name != 'pull_request'
    runs-on: ubuntu-latest
    timeout-minutes: 15
    outputs:
      stale_count: ${{ steps.stale.outputs.stale_count }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download scan results
        uses: actions/download-artifact@v4
        with:
          name: todo-scan-results

      - name: Detect stale TODOs via git blame
        id: stale
        env:
          STALE_DAYS: ${{ github.event.inputs.stale_threshold_days || '90' }}
        run: |
          set -euo pipefail

          python3 << 'PYEOF'
          import json
          import subprocess
          import os
          from datetime import datetime, timezone, timedelta

          stale_days = int(os.environ.get('STALE_DAYS', '90'))
          cutoff = datetime.now(timezone.utc) - timedelta(days=stale_days)
          summary_file = os.environ['GITHUB_STEP_SUMMARY']

          if not os.path.exists('todo-scan-results.json'):
              with open(os.environ['GITHUB_OUTPUT'], 'a') as out:
                  out.write('stale_count=0\n')
              with open(summary_file, 'a') as f:
                  f.write('## Stale TODO Detection\n\nNo scan results available.\n')
              raise SystemExit(0)

          with open('todo-scan-results.json') as f:
              data = json.load(f)

          stale_todos = []

          for todo in data.get('todos', []):
              file_path = todo.get('file', '')
              line_num = todo.get('line', 0)

              if not file_path or not os.path.exists(file_path):
                  continue

              try:
                  result = subprocess.run(
                      ['git', 'blame', '-L', f'{line_num},{line_num}', '--porcelain', file_path],
                      capture_output=True, text=True, timeout=10
                  )
                  if result.returncode != 0:
                      continue

                  author_time = None
                  author = None
                  for blame_line in result.stdout.splitlines():
                      if blame_line.startswith('author-time '):
                          timestamp = int(blame_line.split(' ', 1)[1])
                          author_time = datetime.fromtimestamp(timestamp, tz=timezone.utc)
                      elif blame_line.startswith('author '):
                          author = blame_line.split(' ', 1)[1]

                  if author_time and author_time < cutoff:
                      age_days = (datetime.now(timezone.utc) - author_time).days
                      stale_todos.append({
                          'type': todo.get('type', 'TODO'),
                          'text': todo.get('text', '')[:80],
                          'file': file_path,
                          'line': line_num,
                          'author': author or 'unknown',
                          'date': author_time.strftime('%Y-%m-%d'),
                          'age_days': age_days,
                          'priority': todo.get('priority', 'normal'),
                      })
              except (subprocess.TimeoutExpired, Exception):
                  continue

          stale_todos.sort(key=lambda x: -x['age_days'])

          with open(os.environ['GITHUB_OUTPUT'], 'a') as out:
              out.write(f'stale_count={len(stale_todos)}\n')

          # Save stale list for other jobs
          with open('stale-todos.json', 'w') as f:
              json.dump(stale_todos, f, indent=2)

          lines = [
              '## Stale TODO Detection',
              '',
              f'**Threshold:** {stale_days} days',
              f'**Stale TODOs found:** {len(stale_todos)}',
              '',
          ]

          if stale_todos:
              lines.extend([
                  '### Oldest TODOs',
                  '',
                  '| Age (days) | Type | File | Author | Description |',
                  '|------------|------|------|--------|-------------|',
              ])
              for todo in stale_todos[:20]:
                  desc = todo["text"][:50] + ("..." if len(todo["text"]) > 50 else "")
                  lines.append(
                      f'| {todo["age_days"]} | `{todo["type"]}` '
                      f'| `{todo["file"]}:{todo["line"]}` '
                      f'| {todo["author"]} | {desc} |'
                  )

              if len(stale_todos) > 20:
                  lines.append(f'| ... | ... | ... | ... | _and {len(stale_todos) - 20} more_ |')

              # Age distribution
              buckets = {'90-180 days': 0, '180-365 days': 0, '1+ years': 0}
              for t in stale_todos:
                  if t['age_days'] < 180:
                      buckets['90-180 days'] += 1
                  elif t['age_days'] < 365:
                      buckets['180-365 days'] += 1
                  else:
                      buckets['1+ years'] += 1

              lines.extend([
                  '',
                  '### Age Distribution',
                  '',
                  '| Range | Count |',
                  '|-------|-------|',
              ])
              for label, count in buckets.items():
                  if count > 0:
                      lines.append(f'| {label} | {count} |')
          else:
              lines.append('No TODOs older than the threshold were found.')

          with open(summary_file, 'a') as f:
              f.write('\n'.join(lines) + '\n')
          PYEOF

      - name: Upload stale TODO data
        uses: actions/upload-artifact@v4
        with:
          name: stale-todos
          path: stale-todos.json
          retention-days: 30

  # â”€â”€â”€ Resolved Issue Audit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  resolved-issue-audit:
    name: Resolved Issue Audit
    needs: scan-todos
    if: github.event_name != 'pull_request'
    runs-on: ubuntu-latest
    timeout-minutes: 10
    outputs:
      orphaned_count: ${{ steps.audit.outputs.orphaned_count }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download scan results
        uses: actions/download-artifact@v4
        with:
          name: todo-scan-results

      - name: Audit TODOs with closed issues
        id: audit
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euo pipefail

          python3 << 'PYEOF'
          import json
          import subprocess
          import os

          summary_file = os.environ['GITHUB_STEP_SUMMARY']

          if not os.path.exists('todo-scan-results.json'):
              with open(os.environ['GITHUB_OUTPUT'], 'a') as out:
                  out.write('orphaned_count=0\n')
              with open(summary_file, 'a') as f:
                  f.write('## Resolved Issue Audit\n\nNo scan results available.\n')
              raise SystemExit(0)

          with open('todo-scan-results.json') as f:
              data = json.load(f)

          # Collect all referenced issue numbers
          issue_refs = set()
          for todo in data.get('todos', []):
              for ref in todo.get('issue_refs', []):
                  try:
                      issue_refs.add(int(ref))
                  except (ValueError, TypeError):
                      pass

          if not issue_refs:
              with open(os.environ['GITHUB_OUTPUT'], 'a') as out:
                  out.write('orphaned_count=0\n')
              with open(summary_file, 'a') as f:
                  f.write('## Resolved Issue Audit\n\nNo TODOs reference GitHub issues. Nothing to audit.\n')
              raise SystemExit(0)

          # Check status of each referenced issue
          closed_issues = {}
          for issue_num in sorted(issue_refs):
              try:
                  result = subprocess.run(
                      ['gh', 'issue', 'view', str(issue_num), '--json', 'state,title,closedAt'],
                      capture_output=True, text=True, timeout=15
                  )
                  if result.returncode == 0:
                      issue_data = json.loads(result.stdout)
                      if issue_data.get('state') == 'CLOSED':
                          closed_issues[issue_num] = {
                              'title': issue_data.get('title', ''),
                              'closed_at': issue_data.get('closedAt', '')[:10],
                          }
              except (subprocess.TimeoutExpired, json.JSONDecodeError, Exception):
                  continue

          # Find TODOs that reference closed issues
          orphaned = []
          for todo in data.get('todos', []):
              for ref in todo.get('issue_refs', []):
                  try:
                      ref_num = int(ref)
                  except (ValueError, TypeError):
                      continue
                  if ref_num in closed_issues:
                      orphaned.append({
                          'type': todo.get('type', 'TODO'),
                          'text': todo.get('text', '')[:60],
                          'file': todo.get('file', ''),
                          'line': todo.get('line', 0),
                          'issue': ref_num,
                          'issue_title': closed_issues[ref_num]['title'][:50],
                          'closed_at': closed_issues[ref_num]['closed_at'],
                      })

          with open(os.environ['GITHUB_OUTPUT'], 'a') as out:
              out.write(f'orphaned_count={len(orphaned)}\n')

          with open('orphaned-todos.json', 'w') as f:
              json.dump(orphaned, f, indent=2)

          lines = [
              '## Resolved Issue Audit',
              '',
              f'**Issues referenced by TODOs:** {len(issue_refs)}',
              f'**Closed issues still in code:** {len(closed_issues)}',
              f'**Orphaned TODO comments:** {len(orphaned)}',
              '',
          ]

          if orphaned:
              lines.extend([
                  '### Orphaned TODOs (linked issue is closed)',
                  '',
                  'These TODO comments reference issues that have been closed. '
                  'The TODO may be stale and eligible for removal.',
                  '',
                  '| File | Type | Issue | Closed | Description |',
                  '|------|------|-------|--------|-------------|',
              ])
              for item in orphaned:
                  lines.append(
                      f'| `{item["file"]}:{item["line"]}` '
                      f'| `{item["type"]}` '
                      f'| #{item["issue"]} '
                      f'| {item["closed_at"]} '
                      f'| {item["text"]} |'
                  )
              lines.extend([
                  '',
                  '> **Action:** Review these TODOs and remove them if the linked issue '
                  'has been fully resolved, or re-open the issue if work remains.',
              ])
          else:
              lines.append('All TODO-linked issues are still open. No orphaned TODOs found.')

          with open(summary_file, 'a') as f:
              f.write('\n'.join(lines) + '\n')
          PYEOF

      - name: Upload orphaned TODO data
        uses: actions/upload-artifact@v4
        with:
          name: orphaned-todos
          path: orphaned-todos.json
          retention-days: 30

  # â”€â”€â”€ PR TODO Diff â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  pr-todo-diff:
    name: PR TODO Diff
    if: github.event_name == 'pull_request'
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout PR
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Detect new TODOs in PR diff
        id: diff
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euo pipefail

          python3 << 'PYEOF'
          import re
          import subprocess
          import json
          import os

          summary_file = os.environ['GITHUB_STEP_SUMMARY']

          # Get the diff against the base branch
          base_ref = os.environ.get('GITHUB_BASE_REF', 'main')
          result = subprocess.run(
              ['git', 'diff', f'origin/{base_ref}...HEAD', '--unified=0', '--diff-filter=AM'],
              capture_output=True, text=True
          )

          if result.returncode != 0:
              with open(os.environ['GITHUB_OUTPUT'], 'a') as out:
                  out.write('new_todo_count=0\n')
                  out.write('comment_body=\n')
              raise SystemExit(0)

          diff_text = result.stdout

          # Parse added lines from the diff
          TODO_TYPES = ['TODO', 'FIXME', 'HACK', 'BUG', 'XXX', 'PERF', 'OPTIMIZE', 'REFACTOR', 'NOTE']
          types_pattern = '|'.join(TODO_TYPES)
          todo_regex = re.compile(rf'\b({types_pattern})[\s:]+(.+)', re.IGNORECASE)

          current_file = None
          new_todos = []

          for line in diff_text.splitlines():
              if line.startswith('diff --git'):
                  # Extract file path: diff --git a/path b/path
                  parts = line.split(' b/')
                  current_file = parts[-1] if len(parts) > 1 else None
              elif line.startswith('+') and not line.startswith('+++'):
                  added_content = line[1:]
                  match = todo_regex.search(added_content)
                  if match:
                      new_todos.append({
                          'file': current_file or 'unknown',
                          'type': match.group(1).upper(),
                          'text': match.group(2).strip()[:120],
                      })

          with open(os.environ['GITHUB_OUTPUT'], 'a') as out:
              out.write(f'new_todo_count={len(new_todos)}\n')

          # Build PR comment
          if new_todos:
              comment_lines = [
                  '## ðŸ“‹ New TODO Comments Detected',
                  '',
                  f'This PR introduces **{len(new_todos)}** new TODO comment(s):',
                  '',
                  '| Type | File | Description |',
                  '|------|------|-------------|',
              ]
              for t in new_todos:
                  desc = t["text"][:80] + ("..." if len(t["text"]) > 80 else "")
                  comment_lines.append(f'| `{t["type"]}` | `{t["file"]}` | {desc} |')

              comment_lines.extend([
                  '',
                  '<details><summary>Guidelines</summary>',
                  '',
                  '- Link TODOs to GitHub issues: `// TODO: Track with issue #123`',
                  '- Use `FIXME` for bugs, `HACK` for temporary workarounds',
                  '- Consider creating an issue for non-trivial TODOs',
                  '',
                  '</details>',
                  '',
                  '_Reported by TODO Automation workflow._',
              ])

              comment_body = '\n'.join(comment_lines)

              # Write comment body to file for the next step
              with open('pr-comment.md', 'w') as f:
                  f.write(comment_body)
          else:
              with open('pr-comment.md', 'w') as f:
                  f.write('')

          # Step summary
          lines = [
              '## PR TODO Diff',
              '',
              f'**New TODOs in this PR:** {len(new_todos)}',
              '',
          ]
          if new_todos:
              lines.extend([
                  '| Type | File | Description |',
                  '|------|------|-------------|',
              ])
              for t in new_todos:
                  desc = t["text"][:80] + ("..." if len(t["text"]) > 80 else "")
                  lines.append(f'| `{t["type"]}` | `{t["file"]}` | {desc} |')
          else:
              lines.append('No new TODO comments introduced in this PR.')

          with open(summary_file, 'a') as f:
              f.write('\n'.join(lines) + '\n')
          PYEOF

      - name: Comment on PR with new TODOs
        if: hashFiles('pr-comment.md') != '' && steps.diff.outputs.new_todo_count != '0'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Check for existing TODO comment to avoid duplicates
          EXISTING=$(gh pr view "${{ github.event.pull_request.number }}" \
            --json comments --jq '.comments[].body' 2>/dev/null | \
            grep -c "New TODO Comments Detected" || true)

          if [ "$EXISTING" -gt 0 ]; then
            echo "TODO comment already exists on this PR, skipping."
          else
            gh pr comment "${{ github.event.pull_request.number }}" \
              --body-file pr-comment.md
          fi

  # â”€â”€â”€ Duplicate Detection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  duplicate-detection:
    name: Duplicate TODO Detection
    needs: scan-todos
    if: github.event_name != 'pull_request'
    runs-on: ubuntu-latest
    timeout-minutes: 10
    outputs:
      duplicate_groups: ${{ steps.dupes.outputs.duplicate_groups }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download scan results
        uses: actions/download-artifact@v4
        with:
          name: todo-scan-results

      - name: Detect duplicate TODOs
        id: dupes
        run: |
          set -euo pipefail

          python3 << 'PYEOF'
          import json
          import os
          import re
          from collections import defaultdict

          summary_file = os.environ['GITHUB_STEP_SUMMARY']

          if not os.path.exists('todo-scan-results.json'):
              with open(os.environ['GITHUB_OUTPUT'], 'a') as out:
                  out.write('duplicate_groups=0\n')
              with open(summary_file, 'a') as f:
                  f.write('## Duplicate Detection\n\nNo scan results available.\n')
              raise SystemExit(0)

          with open('todo-scan-results.json') as f:
              data = json.load(f)

          todos = data.get('todos', [])

          def normalize(text):
              """Normalize text for comparison: lowercase, strip punctuation, collapse whitespace."""
              text = text.lower().strip()
              text = re.sub(r'[^\w\s]', '', text)
              text = re.sub(r'\s+', ' ', text)
              # Remove common prefixes like issue references
              text = re.sub(r'track with issue \d+', '', text)
              text = re.sub(r'issue \d+', '', text)
              return text.strip()

          def tokenize(text):
              """Create a set of significant tokens for comparison."""
              stop_words = {'the', 'a', 'an', 'is', 'are', 'was', 'be', 'to', 'of', 'and',
                           'in', 'for', 'on', 'with', 'this', 'that', 'it', 'not', 'but',
                           'we', 'need', 'should', 'could', 'would', 'will', 'can'}
              words = normalize(text).split()
              return {w for w in words if w not in stop_words and len(w) > 2}

          def similarity(tokens_a, tokens_b):
              """Jaccard similarity between two token sets."""
              if not tokens_a or not tokens_b:
                  return 0.0
              intersection = tokens_a & tokens_b
              union = tokens_a | tokens_b
              return len(intersection) / len(union) if union else 0.0

          # Index all TODOs by their tokens
          indexed = []
          for i, todo in enumerate(todos):
              tokens = tokenize(todo.get('text', ''))
              if len(tokens) >= 2:  # skip very short texts
                  indexed.append((i, tokens, todo))

          # Find duplicate groups (similarity >= 0.6)
          THRESHOLD = 0.6
          visited = set()
          groups = []

          for i, (idx_a, tokens_a, todo_a) in enumerate(indexed):
              if idx_a in visited:
                  continue
              group = [todo_a]
              for j, (idx_b, tokens_b, todo_b) in enumerate(indexed):
                  if idx_b in visited or idx_a == idx_b:
                      continue
                  if similarity(tokens_a, tokens_b) >= THRESHOLD:
                      group.append(todo_b)
                      visited.add(idx_b)
              if len(group) > 1:
                  visited.add(idx_a)
                  groups.append(group)

          with open(os.environ['GITHUB_OUTPUT'], 'a') as out:
              out.write(f'duplicate_groups={len(groups)}\n')

          with open('duplicate-todos.json', 'w') as f:
              json.dump([
                  [{'type': t.get('type'), 'text': t.get('text', '')[:80],
                    'file': t.get('file'), 'line': t.get('line')} for t in g]
                  for g in groups
              ], f, indent=2)

          lines = [
              '## Duplicate TODO Detection',
              '',
              f'**Total TODOs analyzed:** {len(indexed)}',
              f'**Duplicate groups found:** {len(groups)}',
              f'**Similarity threshold:** {THRESHOLD:.0%}',
              '',
          ]

          if groups:
              lines.append('### Duplicate Groups')
              lines.append('')
              for gi, group in enumerate(groups[:10], 1):
                  lines.append(f'**Group {gi}** ({len(group)} items):')
                  for item in group:
                      text = item.get("text", "")[:60]
                      lines.append(
                          f'- `{item.get("type", "TODO")}` in `{item.get("file", "?")}:{item.get("line", 0)}` â€” {text}'
                      )
                  lines.append('')

              if len(groups) > 10:
                  lines.append(f'_...and {len(groups) - 10} more group(s)._')

              lines.extend([
                  '',
                  '> **Recommendation:** Consider consolidating duplicate TODOs into '
                  'a single tracked issue to reduce noise.',
              ])
          else:
              lines.append('No duplicate or highly similar TODOs detected.')

          with open(summary_file, 'a') as f:
              f.write('\n'.join(lines) + '\n')
          PYEOF

      - name: Upload duplicate data
        uses: actions/upload-artifact@v4
        with:
          name: duplicate-todos
          path: duplicate-todos.json
          retention-days: 30

  # â”€â”€â”€ Hotspot Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  hotspot-analysis:
    name: TODO Hotspot Analysis
    needs: scan-todos
    if: github.event_name != 'pull_request'
    runs-on: ubuntu-latest
    timeout-minutes: 10
    outputs:
      top_hotspot: ${{ steps.hotspots.outputs.top_hotspot }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download scan results
        uses: actions/download-artifact@v4
        with:
          name: todo-scan-results

      - name: Analyze TODO hotspots
        id: hotspots
        run: |
          set -euo pipefail

          python3 << 'PYEOF'
          import json
          import os
          import subprocess
          from collections import defaultdict
          from pathlib import Path

          summary_file = os.environ['GITHUB_STEP_SUMMARY']

          if not os.path.exists('todo-scan-results.json'):
              with open(os.environ['GITHUB_OUTPUT'], 'a') as out:
                  out.write('top_hotspot=none\n')
              with open(summary_file, 'a') as f:
                  f.write('## TODO Hotspot Analysis\n\nNo scan results available.\n')
              raise SystemExit(0)

          with open('todo-scan-results.json') as f:
              data = json.load(f)

          todos = data.get('todos', [])

          # === By file ===
          by_file = defaultdict(list)
          for todo in todos:
              by_file[todo.get('file', 'unknown')].append(todo)

          # Calculate density: TODOs / file lines
          file_density = []
          for file_path, items in by_file.items():
              try:
                  if os.path.exists(file_path):
                      line_count = sum(1 for _ in open(file_path, encoding='utf-8', errors='replace'))
                      density = len(items) / max(line_count, 1)
                  else:
                      line_count = 0
                      density = 0
              except Exception:
                  line_count = 0
                  density = 0

              file_density.append({
                  'file': file_path,
                  'count': len(items),
                  'lines': line_count,
                  'density': density,
                  'types': list(set(t.get('type', 'TODO') for t in items)),
                  'high_priority': sum(1 for t in items if t.get('priority') == 'high'),
              })

          # Sort by count desc, then density desc
          file_density.sort(key=lambda x: (-x['count'], -x['density']))

          # === By directory (2-level deep) ===
          by_dir = defaultdict(lambda: {'count': 0, 'files': set(), 'types': defaultdict(int)})
          for todo in todos:
              parts = Path(todo.get('file', '')).parts
              dir_key = '/'.join(parts[:3]) if len(parts) >= 3 else '/'.join(parts[:2]) if len(parts) >= 2 else 'root'
              by_dir[dir_key]['count'] += 1
              by_dir[dir_key]['files'].add(todo.get('file', ''))
              by_dir[dir_key]['types'][todo.get('type', 'TODO')] += 1

          dir_hotspots = sorted(by_dir.items(), key=lambda x: -x[1]['count'])

          top_hotspot = file_density[0]['file'] if file_density else 'none'
          with open(os.environ['GITHUB_OUTPUT'], 'a') as out:
              out.write(f'top_hotspot={top_hotspot}\n')

          lines = [
              '## TODO Hotspot Analysis',
              '',
              f'**Files with TODOs:** {len(by_file)}',
              f'**Directories with TODOs:** {len(by_dir)}',
              '',
          ]

          # Top files by count
          if file_density:
              lines.extend([
                  '### Top Files by TODO Count',
                  '',
                  '| File | TODOs | Lines | Density | High Priority | Types |',
                  '|------|-------|-------|---------|---------------|-------|',
              ])
              for fd in file_density[:15]:
                  density_pct = f'{fd["density"]*100:.1f}%'
                  types_str = ', '.join(fd['types'])
                  lines.append(
                      f'| `{fd["file"]}` | {fd["count"]} | {fd["lines"]} '
                      f'| {density_pct} | {fd["high_priority"]} | {types_str} |'
                  )
              lines.append('')

          # Top directories
          if dir_hotspots:
              lines.extend([
                  '### Top Directories by TODO Count',
                  '',
                  '| Directory | TODOs | Files | Dominant Type |',
                  '|-----------|-------|-------|---------------|',
              ])
              for dir_name, info in dir_hotspots[:10]:
                  dominant = max(info['types'].items(), key=lambda x: x[1])[0] if info['types'] else 'N/A'
                  lines.append(
                      f'| `{dir_name}/` | {info["count"]} | {len(info["files"])} | `{dominant}` |'
                  )
              lines.append('')

          # High-density warning
          high_density = [f for f in file_density if f['density'] > 0.05 and f['count'] >= 3]
          if high_density:
              lines.extend([
                  '### High-Density Files (>5% of lines are TODOs)',
                  '',
              ])
              for fd in high_density[:5]:
                  density_pct = f'{fd["density"]*100:.1f}%'
                  lines.append(f'- `{fd["file"]}`: {fd["count"]} TODOs in {fd["lines"]} lines ({density_pct})')
              lines.extend([
                  '',
                  '> **Action:** These files have a high concentration of TODOs and may '
                  'benefit from focused refactoring attention.',
              ])

          with open(summary_file, 'a') as f:
              f.write('\n'.join(lines) + '\n')
          PYEOF

  # â”€â”€â”€ Create Issues â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  create-issues:
    name: Create Issues for Untracked TODOs
    needs: scan-todos
    if: |
      github.event.inputs.create_issues == 'true' &&
      needs.scan-todos.outputs.new_todos > 0
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download scan results
        uses: actions/download-artifact@v4
        with:
          name: todo-scan-results

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Create issues for untracked TODOs
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          python3 << 'EOF'
          import json
          import subprocess
          import os

          with open('todo-scan-results.json', 'r') as f:
              data = json.load(f)

          created_count = 0
          max_issues = 5  # Limit to prevent spam

          for todo in data.get('todos', []):
              if created_count >= max_issues:
                  print(f"Reached maximum issue limit ({max_issues})")
                  break

              if todo.get('has_issue', False):
                  continue

              todo_type = todo.get('type', 'TODO')
              file_path = todo.get('file', 'unknown')
              line = todo.get('line', 0)
              text = todo.get('text', 'No description')

              # Determine labels
              labels = ['todo-automation']
              if todo_type == 'FIXME':
                  labels.append('bug')
              elif todo_type == 'HACK':
                  labels.append('tech-debt')

              title = f"[{todo_type}] {text[:60]}{'...' if len(text) > 60 else ''}"

              body = f"""## TODO Item

          **Type:** {todo_type}
          **File:** `{file_path}`
          **Line:** {line}

          ### Description

          {text}

          ### Context

          ```
          {todo.get('context', 'No context available')}
          ```

          ---
          *This issue was automatically created by the TODO Automation workflow.*
          """

              try:
                  result = subprocess.run([
                      'gh', 'issue', 'create',
                      '--title', title,
                      '--body', body,
                      '--label', ','.join(labels)
                  ], capture_output=True, text=True, check=True)

                  print(f"Created issue: {result.stdout.strip()}")
                  created_count += 1
              except subprocess.CalledProcessError as e:
                  print(f"Failed to create issue: {e.stderr}")

          print(f"\nCreated {created_count} issues")

          with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as f:
              f.write(f"\n\n## Issues Created\n\nCreated **{created_count}** new issues for untracked TODOs.\n")
          EOF

  # â”€â”€â”€ Notification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  notify:
    name: Send Notification
    needs:
      - scan-todos
      - trend-tracking
      - stale-todo-detection
      - resolved-issue-audit
      - duplicate-detection
      - hotspot-analysis
      - create-issues
    if: always() && needs.scan-todos.outputs.total_count > 0 && github.event_name != 'pull_request'
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
      - name: Summary notification
        run: |
          echo "## TODO Automation Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### Core Scan" >> $GITHUB_STEP_SUMMARY
          echo "- **Scan Status:** ${{ needs.scan-todos.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Total TODOs:** ${{ needs.scan-todos.outputs.total_count }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Untracked:** ${{ needs.scan-todos.outputs.new_todos }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Documentation Updated:** ${{ needs.scan-todos.outputs.has_changes }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### Trend Analysis" >> $GITHUB_STEP_SUMMARY
          echo "- **Trend Direction:** ${{ needs.trend-tracking.outputs.trend_direction || 'N/A' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Change (delta):** ${{ needs.trend-tracking.outputs.delta || '0' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### Stale TODO Detection" >> $GITHUB_STEP_SUMMARY
          echo "- **Stale TODOs (>${{ github.event.inputs.stale_threshold_days || '90' }} days):** ${{ needs.stale-todo-detection.outputs.stale_count || '0' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### Resolved Issue Audit" >> $GITHUB_STEP_SUMMARY
          echo "- **Orphaned TODOs (closed issues):** ${{ needs.resolved-issue-audit.outputs.orphaned_count || '0' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### Duplicate Detection" >> $GITHUB_STEP_SUMMARY
          echo "- **Duplicate groups:** ${{ needs.duplicate-detection.outputs.duplicate_groups || '0' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### Hotspot Analysis" >> $GITHUB_STEP_SUMMARY
          echo "- **Top hotspot:** \`${{ needs.hotspot-analysis.outputs.top_hotspot || 'N/A' }}\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ github.event.inputs.create_issues }}" == "true" ]; then
            echo "### Issue Creation" >> $GITHUB_STEP_SUMMARY
            echo "- **Status:** ${{ needs.create-issues.result }}" >> $GITHUB_STEP_SUMMARY
          fi
